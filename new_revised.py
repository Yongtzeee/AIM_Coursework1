# customized code
# -*- coding: utf-8 -*-
"""gbndm_aim_0_3_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ny3YOI8G7xTqCFgCdqpdkZvI6yrfwdIe
"""

#version = '_v0p3p9_'

from __future__ import print_function
import argparse
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import time
import warnings

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab

from sklearn import cluster, datasets, mixture
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler
from itertools import cycle, islice

import torch
import numpy as np
from torch.utils import data
import copy

from torchvision import datasets, transforms
from torch.autograd import Variable

from IPython.core.debugger import set_trace

# ===============
# Initializations
# ===============

#!pip install torch torchvision

# -----------------------------------------------------------------------------------------------------------

# ===============
# Parameters
# ===============

# --- General framework arguments

args = {}
kwargs = {}
args['num_train_batch'] = 1  # number of MNIST training batches
args['num_valid_batch'] = 10  # number of MNIST validation batches
args['train_batch_size'] = 100   # training batch size
args['valid_batch_size'] = 100   # validation batch size
args['test_batch_size'] = 10
# args['epochs'] = 100  # number of training epochs 
args['lr'] = 1 # 0.1  # 0.01 # learning rate # this is over-written by the solution
args['momentum'] = 0.5 # SGD momentum (default: 0.5); momentum is a moving average of our gradients (helps keep a useful direction)
args['clip_level'] = 0.5  # gradient clip threshold
args['seed']= 1 #random seed
args['log_interval_epoch'] = 1 # display training loss every log_int... epochs
args['cuda'] = True 
args['patience'] = 1000  # stop train. if the valid. err. hasn't improved by this num. of epochs
args['noise_in'] = -1   # 0.5  # 0.15  # amount of noise to add to the training data
args['noise_out'] = -1   #  probability of changing an output label to some random label
args['verbose_train'] = False   # print status of model training?
args['verbose_meta'] = True #  print status of architecture optimization?
args['min_inst_class'] = 5 # minimum number of instances per class in the training set
args['batch_max_tries'] = 10 # max. num. of attempts in extracting data batches
args['save_best_chrom'] = False # False  # save the best chromose in Google Drive?
args['dc_prob_drop'] = 0.5 # 0.5 # probability of dropping a circuit (DropCircuit)
args['prob_sel_branch'] = 0.5 # probability of architectural search selecting/using a branch (this is not DropCircuit) 

# --- Neural architectural limits

args['num_epochs_search'] = 10 # 100  # number of epochs for training during architecture search 
args['num_epochs_test'] = 1000  # num. epochs for training during the final test
limits = {}
limits['min_layer_nodes'] =  10 # 5 # 50
limits['max_layer_nodes'] = 100
limits['max_pre_branch_layers'] = 5 # 3
limits['max_branches'] = 10
limits['max_branch_layers'] = 5  # 3
limits['max_post_branch_layers'] = 5 # 3
limits['lr1_min'] = 0.001
limits['lr1_,max'] = 2
limits['lr2_min'] = 0.001
limits['lr2_,max'] = 2
limits['moment_min'] = 0.001
limits['moment_max'] = 1

# np.random.seed(0)
# torch.manual_seed(0)

num_train_instances = args['num_train_batch'] * args['train_batch_size']
num_valid_instances = args['num_valid_batch'] * args['valid_batch_size']

data_rand_seed = 1 # (other pre-tested seeds: 2, 3)
do_eval_iter = 0

# -----------------------------------------------------------------------------------------------------------

# ============
# Load dataset
# ============

# Seed creation
torch.manual_seed(data_rand_seed)
np.random.seed(data_rand_seed)

a_data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
  
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                   transform=a_data_transform),
    batch_size=args['train_batch_size'], shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=a_data_transform),
    batch_size=args['test_batch_size'], shuffle=True, **kwargs)

# Check whether we have enough instances per class
# We want an imbalanced dataset but we don't want any specific label having
# an "insufficient" number of instances.
def check_enough_inst(batches, min_inst_per_class):
  
  # Concatenate training batch output labels
  labels = batches[0][1].numpy()
  for batch_i in range(1,args['num_train_batch']):
    new_labels = batches[batch_i][1].numpy()
    labels = np.concatenate((labels, new_labels))

  labels = labels.tolist()

  # Scan though labels
  for a_label in range(10):  # assuming MNIST, of course
    a_count = labels.count(a_label)
    # If label count is < min_inst_per_class return False
    if a_count < min_inst_per_class:
      return False

  # Return True
  return True


# Prepare data. Extract training and validation batches
# This is where we make the problem "small and imbalanced"
def extract_batches(a_loader, min_inst_per_class, max_tries):

  # Keep trying until you have a required minimum number of instances 
  # per class in the training set (not elegant but ok for the range of 
  # "min_inst_per_class" we need)

  for ti in range(max_tries):

    print('Data extraction trial {}.'.format(ti))

    # Initializations
    train_batches = []
    valid_batches = []
    tot_batch_extract = args['num_train_batch'] + args['num_valid_batch']

    # Extract
    for batch_idx, (data, target) in enumerate(a_loader):

      if batch_idx < args['num_train_batch']:
        train_batches.append((data,target))
      else:
        valid_batches.append((data,target))
        if batch_idx == tot_batch_extract - 1:
          break

    # Check minimum number of instances
    enough_instaces = check_enough_inst(train_batches, min_inst_per_class)
    if enough_instaces:
      return train_batches, valid_batches

  print('It was not possible to create a dataset.')
  print('Consider increasing the overall number of instances, or')
  print('decreasing the minimum instances per class allowed.')
  return [], []


train_batches, valid_batches = extract_batches(train_loader, args['min_inst_class'], args['batch_max_tries'])
if args['verbose_train']:
  print('Extracted {} train_batches, and {} valid_batches.'.format(len(train_batches), len(valid_batches)))

# -----------------------------------------------------------------------------------------------------------

# ===========================
# Display histogram of labels
# ===========================

def disp_hist_labaels(batches):
  # Concatenate training batch output labels
  labels = batches[0][1].numpy()		# batches[0][0] are the pictures, batches[0][1] are labels for each picture
  for batch_i in range(1,args['num_train_batch']):
    new_labels = batches[batch_i][1].numpy()
    labels = np.concatenate((labels, new_labels))

  num_bins = 10
  n, bins, patches = plt.hist(labels, num_bins, facecolor='blue', alpha=0.5)
  plt.show()

disp_hist_labaels(train_batches)

# ==========================
# Display dataset
# ==========================

# This is not currently used

# Display MNIST instances
# Adapted from # https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb
def disp_MNIST_inst(num_disp, X_train, y_train):
  pltsize=1
  plt.figure(figsize=(num_disp*pltsize, pltsize))
  for i in range(num_disp):
    plt.subplot(1,num_disp,i+1)
    plt.axis('off')
    plt.imshow(X_train[i,:,:].numpy().reshape(28,28), cmap="gray_r")
    plt.title('Class: '+str(y_train[i].item()))

if args['verbose_train']:
  X_train = train_batches[0][0]
  y_train = train_batches[0][1]

  disp_MNIST_inst(10, X_train, y_train)

  sum_train_0 = X_train[0,:,:].sum()
  min_train_0 = X_train[0,:,:].min()
  max_train_0 = X_train[0,:,:].max()

  print('X_train[0,:,:] --> sum ({}); min ({}); max ({}).'.format(sum_train_0, min_train_0, max_train_0))

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Design model
# ==========================

# "Unseed" the rest
np.random.seed()
a_rand_seed = np.random.randint(0,999999)
torch.manual_seed(a_rand_seed)

# Simple function to scale parameters
# num is assume to be \in [0,1]
def scale_to_range(num,min_val,max_val):
  range = max_val - min_val
  return (num*range)+min_val

# Simple layer for doing elementwise scaling
# Adapated from https://stackoverflow.com/questions/51980654/pytorch-element-wise-filter-layer
class Elem_Scaling_1D(nn.Module):
  def __init__(self, num_nodes, bogus):  # clean-up "bogus"
    super(Elem_Scaling_1D, self).__init__()
    # Initialize
    init_weights = np.random.normal(loc=0, scale=0.5, size=np.shape(num_nodes))
    # Assign
    self.weights = torch.from_numpy(init_weights)
    #self.weights = nn.Parameter(torch.Tensor(1, num_nodes))  # trainable parameter

  def forward(self, x):
    # assuming x is of size num_inst-1-num_nodes
    return x * self.weights  # element-wise multiplication

# Class gradient-based neural diversity machine
class GBNDM(nn.Module):
    def __init__(self, a_chromosome):   # assuming MNIST
        super(GBNDM, self).__init__()

        # --------- Pre-branch layers 
        self.chromosome = a_chromosome
        self.pre_branch_layers = nn.ModuleList()
        prev_num_out = 28*28
        chrom_ind = 5 # skip over 3 learning rate param., 1 moment. p. (interp./used in training)
        # and 1 exist-or-not (may use in future versions).
        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
        for i in range(limits['max_pre_branch_layers']):
          
          # Extract and interpret parameters
          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
          layer_params_real = self.interp_layer_param(layer_params_raw)
        
          # Decide on whether to create a layer or not
          if i==0:  # the first layer of each segment is done by default
            do_layer = True
          else:
            do_layer = layer_params_real[0]

          if do_layer:
            # Create layer    
            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
            # Append layer and update chromosome index
            self.pre_branch_layers.append(a_layer)
            
          chrom_ind += 6

        # --------- Branches
        num_out_pre_branch = prev_num_out

        self.branches = nn.ModuleList()

        # Scan over branches
        final_num_nodes = []  # keep track of the size of the final layer of each branch
        count_branches = 0
        for bi in range(limits['max_branches']):

          # Initialize branch
          branch_layers = nn.ModuleList()

          # Do branch? Always do the first one by default
          if (a_chromosome[chrom_ind] < args['prob_sel_branch']) or (bi == 0):
            do_branch = True
          else: 
            do_branch = False
            
          chrom_ind += 1

          # Scan over branch layers

          if do_branch:  # if doing branch

            this_fin_num_nodes = 0

            count_branches += 1
            
            for li in range(limits['max_branch_layers']):

              # Extract and interpret parameters
              layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
              layer_params_real = self.interp_layer_param(layer_params_raw)

              # Decide on whether to create a layer or not
              if li==0:  # the first layer of each branch is done by default
                do_layer = True
                prev_num_out = num_out_pre_branch
              else:
                do_layer = layer_params_real[0]

              if do_layer:
                # print('... temp ... this_fin_num_nodes: {}.'.format(this_fin_num_nodes))
                # Create layer    
                a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
                # Num_nodes - keep storing the latest one; the last latest is the final layer num_nodes
                this_fin_num_nodes = prev_num_out
                # Append layer and update chromosome index
                branch_layers.append(a_layer)

              chrom_ind += 6

            # Append branch
            final_num_nodes.append(this_fin_num_nodes)
            #print('final_num_nodes: {}.'.format(final_num_nodes))
            self.branches.append(branch_layers)

          else: # if not doing branch you still need to increment chromosome index

            chrom_ind += 6*limits['max_branch_layers']

        self.num_branches = count_branches
        self.dc_prob_activ = 1 - args['dc_prob_drop'] # probability of using a circuit
        self.tot_nodes_branch_final = sum(final_num_nodes)
        
        # --------- Post-branch layers

        prev_num_out = self.tot_nodes_branch_final
        
        # --- Create post-branch layers

        chrom_ind += 1  # skip over the parameter pertaining to the existence or not of the post-branch segment

        self.post_branch_layers = nn.ModuleList()
        
        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
        for li in range(limits['max_post_branch_layers']):
          
          # Extract and interpret parameters
          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
          layer_params_real = self.interp_layer_param(layer_params_raw)
        
          # Decide on whether to create a layer or not
          if li==0:  # the first layer of each segment is done by default
            do_layer = True
          else:
            do_layer = layer_params_real[0]

          if do_layer:
            # Create layer    
            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
            # Append layer and update chromosome index
            self.post_branch_layers.append(a_layer)
            
          chrom_ind += 6

        # Create a final layer
        self.final_layer = nn.Linear(prev_num_out, 10)

    def forward(self, x, dc_mask = None):
        
        x = x.view(-1, 28*28)

        # Apply pre-branch layers
        for pi, a_layer in enumerate(self.pre_branch_layers):
          #print('Pre-layer {}'.format(pi))
          # if isinstance(a_layer, Elem_Scaling_1D):
          #   set_trace()
          x = a_layer(x)

        # Apply branches
        branch_finals = []
        for bi, a_branch in enumerate(self.branches):
          #print('Branch {}'.format(bi))
          
          z = x

          for a_layer in a_branch:
            # if isinstance(a_layer, Elem_Scaling_1D):
            #   set_trace()
            z = a_layer(z)
          
          if self.training:
            z = (dc_mask[bi] * z) / self.dc_prob_activ  # if training apply DropCircuit
          
          branch_finals.append(z)

        # Concatenate multi-branch final layers
        x = torch.cat(branch_finals,dim=1)

        # if not(self.training): # if not training then testing/evaluating
        #   x = self.dc_prob_activ * x  # scaling due to DropCircuit
      

        # Apply post-branch layers
        for pi, a_layer in enumerate(self.post_branch_layers):
          #print('Post-layer: {}'.format(pi))
          # if isinstance(a_layer, Elem_Scaling_1D):
          #   set_trace()
          x = a_layer(x)

        # Apply final layer
        x = self.final_layer(x)
               
        return F.log_softmax(x, dim=1)


    # Method for interpreting layer parameters
    # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
    def interp_layer_param(self, layer_params_raw):
      # Exist or not
      if layer_params_raw[0] < 0.5:
        exist = False
      else:
        exist = True
      # Weight function
      tot_weight_func = 3
      if layer_params_raw[1] < 0.6: # or ... (1/tot_weight_func):  
        weight_func_sel = 'linear'
      elif layer_params_raw[1] < 0.8: # or ... (2/tot_weight_func):
        weight_func_sel = 'elem_scale'
      else:
        weight_func_sel = 'id'
      # activation function
      tot_node_func = 22
      if layer_params_raw[2] < (1/tot_node_func):
        activ_func = 'ReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (2/tot_node_func):
        activ_func = 'Hardshrink'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (3/tot_node_func):
        activ_func = 'Hardtanh'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,2)
        if param_1 > param_2:  # param_1 is min_val; param_2 is max_val
          tmp_val = param_1
          param_1 = param_2
          param_2 = tmp_val
        elif param_1 == param_2:
          param_2 += 0.1
      elif layer_params_raw[2] < (4/tot_node_func):
        activ_func = 'LeakyReLU'
        param_1 = scale_to_range(layer_params_raw[3],0,1)
        param_2 = -1
      elif layer_params_raw[2] < (5/tot_node_func):
        activ_func = 'LogSigmoid'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (6/tot_node_func):
        activ_func = 'PReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (7/tot_node_func):
        activ_func = 'ELU'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (8/tot_node_func):
        activ_func = 'ReLU6'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (9/tot_node_func):
        activ_func = 'RReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (10/tot_node_func):
        activ_func = 'SELU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (11/tot_node_func):
        activ_func = 'CELU'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (12/tot_node_func):
        activ_func = 'GELU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (13/tot_node_func):
        activ_func = 'Sigmoid'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (14/tot_node_func):
        activ_func = 'Softplus'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,40)
      elif layer_params_raw[2] < (15/tot_node_func):
        activ_func = 'Softshrink'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (16/tot_node_func):
        activ_func = 'Softsign'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (17/tot_node_func):
        activ_func = 'Tanh'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (18/tot_node_func):
        activ_func = 'Tanhshrink'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (19/tot_node_func):
        activ_func = 'Threshold'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,2)
      elif layer_params_raw[2] < (20/tot_node_func):
        activ_func = 'Softmin'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (21/tot_node_func):
        activ_func = 'Softmax'
        param_1 = -1
        param_2 = -1
      else: 
        activ_func = 'None'
        param_1 = -1
        param_2 = -1

      # number of nodes
      num_nodes = scale_to_range(layer_params_raw[5], limits['min_layer_nodes'], limits['max_layer_nodes'])
      num_nodes = num_nodes.astype(int)

      return (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)

    # Method for creating a layer
    # layer_params_real format: (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)
    def create_layer(self, prev_num_out, layer_params_real):
      
      exist, weight_func_sel, activ_func, param_1, param_2, num_nodes = layer_params_real
      
      # wf_param1/wf_param2 --> not elegant 
      if weight_func_sel == 'linear':
        weight_func = nn.Linear
        num_nodes_in = prev_num_out
        num_nodes_out = num_nodes
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
      elif weight_func_sel == 'id':
        weight_func = nn.Identity
        num_nodes_in = prev_num_out
        num_nodes_out = prev_num_out
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
      else:  # 'elem_scale'
        weight_func = Elem_Scaling_1D
        num_nodes_in = prev_num_out
        num_nodes_out = prev_num_out
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
        
      if activ_func == 'ReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU())
      elif activ_func == 'Hardshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Hardshrink(param_1))
      elif activ_func == 'Hardtanh':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Hardtanh(param_1, param_2))
      elif activ_func == 'LeakyReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.LeakyReLU(param_1))
      elif activ_func == 'LogSigmoid':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.LogSigmoid())
      elif activ_func == 'PReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.PReLU())
      elif activ_func == 'ELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ELU(param_1))
      elif activ_func == 'ReLU6':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU6())
      elif activ_func == 'RReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.RReLU())
      elif activ_func == 'SELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.SELU())
      elif activ_func == 'CELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.CELU(param_1))
      elif activ_func == 'GELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU())  # for some reason GELU is not present; revert later if relevant
      elif activ_func == 'Sigmoid':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Sigmoid())
      elif activ_func == 'Softplus':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softplus(param_1, param_2))
      elif activ_func == 'Softshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softplus(param_1))
      elif activ_func == 'Softsign':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softsign())
      elif activ_func == 'Tanh':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Tanh())
      elif activ_func == 'Tanhshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Tanhshrink())
      elif activ_func == 'Threshold':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Threshold(param_1, param_2))
      elif activ_func == 'Softmin':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softmin())
      elif activ_func == 'Softmax':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softmax())
      else:
        a_layer = weight_func(wf_param1, wf_param2)

      return a_layer, num_nodes_out

# -----------------------------------------------------------------------------------------------------------
# Generating chromosomes

# Generate a random chromosome where each param. is \in [0,1)
def gen_rand_chromosome(num_param):
  chrom = np.random.rand(num_param)
  return chrom

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Train and test functions
# ==========================

# Create a random mask for DropCircuit
def make_mask(num_circuits, prob_drop):
  rand_vals = np.random.rand(num_circuits)
  decisions = rand_vals >= prob_drop
  a_mask = np.ones(num_circuits)*decisions
  # Must have at least one circuit active
  if np.sum(a_mask) == 0:
    rand_index = np.random.randint(num_circuits)
    a_mask[rand_index] = 1.0

  return torch.from_numpy(a_mask)

def train_one_epoch(a_model, optimizer, epoch, batches):

  a_model.train()
  for batch_idx, (data, target) in enumerate(batches):
      if args['cuda']:
          data, target = data.cuda(), target.cuda()
      # Variables in Pytorch are differentiable. 
      
      data, target = Variable(data), Variable(target)
      #This will zero out the gradients for this batch. 
      optimizer.zero_grad()
      
      # Create DropCircuit mask
      dc_mask = make_mask(a_model.num_branches, args['dc_prob_drop'])
      
      output = a_model(data, dc_mask)

      # Calculate the negative log likelihood loss.
      loss = F.nll_loss(output, target)
      #dloss/dx for every Variable 
      loss.backward()
      # Gradient clipping
      torch.nn.utils.clip_grad_norm_(a_model.parameters(), args['clip_level'])
      #to do a one-step update on our parameter.
      optimizer.step()
      #Print out the loss periodically. 

  if args['verbose_train']:
    if epoch % args['log_interval_epoch'] == 0:
      print('Epoch: {}. Latest loss: {}.'.format(epoch, loss.data))

# Compute accuracy
# The argument (data_source) of this function can be a data_loader or a list of batches (previously extracted)
def comp_accuracy(a_model, data_source, src_num_instances):
    a_model.eval()
    a_loss = 0
    correct = 0
    preds = torch.zeros(0)
    first = True
    for a_data_in, a_data_out in data_source:
        if args['cuda']:
            a_data_in, a_data_out = a_data_in.cuda(), a_data_out.cuda()
        a_data_in, a_data_out = Variable(a_data_in), Variable(a_data_out)
        output = a_model(a_data_in)

        a_loss += F.nll_loss(output, a_data_out, reduction='sum').data # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        if first:
          preds = pred
          first = False
        else:
          preds = torch.cat((preds, pred),0)

        correct += pred.eq(a_data_out.data.view_as(pred)).long().cpu().sum()

    # Compute and return accuracy
    if type(data_source) == list:  # case: list of batches
      result = 100. * (correct.numpy()/ src_num_instances)
    else: # case: data_loader
      result = 100. * (correct.numpy()/ len(data_source.dataset))
    return result

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Evaluate and Train functions
# ==========================

# Compute the number of parameters in a chromose (depends on limits)
def comp_num_chrom_param(limits):
  # Num. of training parameters
  tot_train_param = 4 # lr1, lr2, lr2 prop, momentum
  # Num. of pre-branch parameters
  tot_pre_branch = 1+(6*limits['max_pre_branch_layers']) # exist-or-not, layer params.
  # Num. of branch parameters
  tot_branch = (1+(6*limits['max_branch_layers']))*limits['max_branches']
  # Num. of post-branch parameters
  tot_post_branch = 1+(6*limits['max_post_branch_layers'])

  # 4 + 31 + 310 + 31 = 376
  return tot_train_param+tot_pre_branch+tot_branch+tot_post_branch

# Interpret learning rate and momentum parameters
def interp_lrm(params):
  lr1 = scale_to_range(params[0], limits['lr1_min'], limits['lr1_,max'])
  lr2 = scale_to_range(params[1], limits['lr2_min'], limits['lr2_,max'])
  if lr1 < lr2:
    temp = lr1
    lr1 = lr2
    lr2 = temp
  lr2_epoch = (np.round(params[2]*args['num_epochs_search'])).astype(int)
  a_decr = (lr1-lr2)/lr2_epoch
  a_momentum = scale_to_range(params[3], limits['moment_min'], limits['moment_max'])
  return lr1, lr2, lr2_epoch, a_momentum, a_decr

# Chromosome evaluation (more efficient than do_training)
# Function to train a specific model
# Early stopping, or returning best validation model, is not implemented 
def do_eval_chrom(a_model, train_params, num_epochs):
  global do_eval_iter
  do_eval_iter += 1

  # Extract basic information
  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  
  args['lr'] = lr1
  args['momentum'] = a_momentum
  
  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])
  valid_accurs = []
  train_accurs = []
  train_start_time = time.time()
  for epoch in range(1, num_epochs + 1):

    if args['verbose_train']:
      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))

    train_one_epoch(a_model, optimizer, epoch, train_batches)
    
    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)
    if args['verbose_train']:
      print('Training accuracy: {}%.'.format(a_train_accur))
    train_accurs.append(a_train_accur)
    
    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)
    if args['verbose_train']:
      print('Validation accuracy: {}%.'.format(a_valid_accur))
    valid_accurs.append(a_valid_accur)

    # Decrement learning rate
    if epoch < lr2_epoch:
        args['lr'] -= lr_decr
        for param_group in optimizer.param_groups:
          param_group['lr'] = args['lr']

  train_elapsed_time = time.time() - train_start_time
  print('The training process took {} seconds.'.format(train_elapsed_time))

  return a_model, valid_accurs, train_accurs

# Function to train a specific model
# Early stopping, or returning best validation model, is not implemented 
def do_training(a_model, train_params, num_epochs):

  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  
  args['lr'] = lr1
  args['momentum'] = a_momentum

  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])
  best_valid_accur = 0
  best_model = type(a_model)(a_model.chromosome) # get a new instance
  valid_accurs = []
  train_accurs = []
  train_start_time = time.time()
  for epoch in range(1, num_epochs + 1):

    if args['verbose_train']:
      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))

    train_one_epoch(a_model, optimizer, epoch, train_batches)
    
    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)
    if args['verbose_train']:
      print('Training accuracy: {}%.'.format(a_train_accur))
    train_accurs.append(a_train_accur)
    
    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)
    if args['verbose_train']:
      print('Validation accuracy: {}%.'.format(a_valid_accur))
    valid_accurs.append(a_valid_accur)
    if a_valid_accur > best_valid_accur:
      best_valid_accur = a_valid_accur
      # best_model.load_state_dict(a_model.state_dict()) # copy weights and stuff
      best_model = copy.deepcopy(a_model)

    # Decrement learning rate
    if epoch < lr2_epoch:
        args['lr'] -= lr_decr
        for param_group in optimizer.param_groups:
          param_group['lr'] = args['lr']

  train_elapsed_time = time.time() - train_start_time
  print('The training process took {} seconds.'.format(train_elapsed_time))

  if args['cuda']:
      best_model.cuda()

  return a_model, best_model, valid_accurs, train_accurs

# -----------------------------------------------------------------------------------------------------------

# ================================
# Architectural search
# ================================

# Artificial Intelligence Methods students should focus on the code below.
# You should keep the neural network code unchanged. This is crucial for 
# comparison purposes. In other words, focus only on modifying the architecture
# search code below.

import random
import math

# --- Architectural search parameters
meta = {}
meta['max_rs_iter'] = 10 # 6 # 10  # initial random search
meta['min_pop_size'] = 5 # minimum population size
meta['num_differential_sol'] = 4 # 8 # number of differential evolution solutions
meta['diff_lr'] = 0.4 # 0.5 # 0.1 # learning rate for differential search

# --- Population based search parameters (genetic algorithm)

meta['local_search_iter'] = 4 # number of local search iterations
meta['step_size'] = 0.1 # step size for local search

meta['diff_search_iter'] = 4

# testing parameters
testing = {}
testing['take_previous'] = 0
testing['take_random'] = 0
# testing['limit_numbers'] = [0 for i in range(10)]

# -----------------------------------------------------------------------------
# preparations functions

# Prepare model
def prepare_model(a_rand_chrom):

  # Initialize chromosome and model
  model = GBNDM(a_rand_chrom)
  
  if args['cuda']:
      model.cuda()

  # Interpret learning rates and momentum
  lr1, lr2, lr2_epoch, a_momentum, lr_decr = interp_lrm(a_rand_chrom[0:4])
  train_params = (lr1, lr2, lr2_epoch, a_momentum, lr_decr)
  args['momentum'] = a_momentum
  if args['verbose_train']:
    print('lr1: {}'.format(lr1))
    print('lr2: {}'.format(lr2))
    print('lr2_epoch: {}'.format(lr2_epoch))
    print('a_momentum: {}'.format(a_momentum))
    print('a_decr: {}'.format(a_decr))

  return model, train_params


# -----------------------------------------------------------------------------
# stochastic hill-climbing algorithm functions

# Evaluate a list of chromosomes
def eval_chromosomes(list_chromosomes,num_chrom_params):
  global do_eval_iter, terminate_search
  
  # best_model_accur, best_chromosome, best_model, best_train_params = best_res
  
  best_model_accur = 0
  best_chromosome = None
  best_model = None
  best_train_params = None

  num_chrom = len(list_chromosomes)
  mat_chrom_acur = np.zeros((num_chrom, 1+num_chrom_params))
  neighb_valid_accurs = []

  for ci, a_chrom in enumerate(list_chromosomes):
    if do_eval_iter >= 250:
      terminate_search = True
      break
    if args['verbose_meta']:
      print('Chromosome {} ...'.format(ci))
    # --- Actual training
    model, train_params = prepare_model(a_chrom)
    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])
    best_valid_accur = max(valid_accurs)
    # Store chromosome and accuracy
    mat_chrom_acur[ci,0] = best_valid_accur
    mat_chrom_acur[ci,1:] = a_chrom
    print('Best validation accuracy: {}%.'.format(best_valid_accur))
    neighb_valid_accurs.append(best_valid_accur)
    if best_valid_accur > best_model_accur:
      best_model_accur = best_valid_accur
      best_chromosome = a_chrom
      best_model = model
      best_train_params = train_params
      if args['verbose_meta']:
        print('*** Improving validation accuracy: {}.'.format(best_model_accur))
  
  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)
  return best_res, mat_chrom_acur


# -----------------------------------------------------------------------------
# differential search functions

def differential_search_v4(chroms_list, bad_chroms_ind, good_solutions, increase_pop, num_chrom_params):
  global best_res, thresh_increase_rate

  new_chromosomes = []
  if increase_pop:
    num_chroms = len(chroms_list)
  else:
    num_chroms = len(bad_chroms_ind)
    if num_chroms > math.floor(len(chroms_list)/2):
      num_chroms = math.floor(len(chroms_list)/2)

  for dsi in range(num_chroms):

    print("=========================================================")
    print("differential search iteration: {}".format(dsi+1))
    print("=========================================================")
    
    
    if len(good_solutions) <= 2:
      solutions_list = np.concatenate((good_solutions, chroms_list))
      random.shuffle(solutions_list)
    else:
      solutions_list = good_solutions
    
    while True:
      rand_chrom1 = random.randint(0, len(solutions_list)-1)
      rand_chrom2 = random.randint(0, len(solutions_list)-1)
      if rand_chrom1 != rand_chrom2:
        break
    rand_chrom1 = solutions_list[rand_chrom1]
    rand_chrom2 = solutions_list[rand_chrom2]

    difference_accur = (rand_chrom1[0] - rand_chrom2[0]) / 100
    difference_vec = np.subtract(rand_chrom1[1:], rand_chrom2[1:])
    directed_vec = np.multiply(difference_vec, difference_accur)
    if increase_pop:
      new_chrom = np.add(random.choice(chroms_list)[1:], directed_vec)
    else:
      new_chrom = np.add(chroms_list[bad_chroms_ind[dsi]][1:], directed_vec)
    np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)

    new_res, new_chrom_accur = eval_chromosomes([new_chrom], num_chrom_params)
    if new_res[0] > best_res[0]:
      thresh_increase_rate += (new_res[0] - best_res[0])/5
      best_res = new_res
    
    if increase_pop:
      new_chromosomes.append(new_chrom_accur[0])
    else:
      if new_chrom_accur[0][0] > chroms_list[bad_chroms_ind[dsi]][0]:
        new_chromosomes.append(new_chrom_accur[0])

  return new_chromosomes


def differential_search_v3(chroms_list, bad_chroms_ind, good_solutions, bad_solutions, increase_pop, num_chrom_params):
  global best_res, thresh_increase_rate

  new_chromosomes = []
  if increase_pop:
    num_chroms = len(chroms_list)
  else:
    num_chroms = len(bad_chroms_ind)
    if num_chroms > math.floor(len(chroms_list)/2):
      num_chroms = math.floor(len(chroms_list)/2)
  # solutions_list = np.concatenate((good_solutions, bad_solutions))
  # random.shuffle(solutions_list)

  # difference between goos_solution mean and best half of population mean
  avg_good_sol = np.mean(good_solutions, 0)
  avg_chroms_list = np.mean(sorted(chroms_list.tolist(), reverse=True)[0:math.floor(len(chroms_list/2))], 0)
  average_chrom = avg_good_sol[1:] - avg_chroms_list[1:]
  avg_accur_diff = (avg_good_sol[0] - avg_chroms_list[0])/100
  # # simple mean
  # average_chrom = np.mean(good_solutions, 0)
  # # weighted mean
  # average_chrom = np.array([0.0 for i in range(num_chrom_params)])
  # ratio = [0.0 for i in range(len(good_solutions))]
  # total = sum([i[0] for i in good_solutions])
  # for i in range(len(good_solutions)):
  #   ratio[i] = good_solutions[i][0]/total
  # for i, chrom in enumerate(good_solutions):
  #   average_chrom += np.multiply(chrom[1:], ratio[i])
  
  for dsi in range(num_chroms):

    print("=========================================================")
    print("differential search iteration: {}".format(dsi+1))
    print("=========================================================")
    
    # while True:
    #   rand_chrom1 = random.randint(0, len(solutions_list)-1)
    #   rand_chrom2 = random.randint(0, len(solutions_list)-1)
    #   if rand_chrom1 != rand_chrom2:
    #     break
    # rand_chrom1 = solutions_list[rand_chrom1]
    # rand_chrom2 = solutions_list[rand_chrom2]
    # difference_accur = (rand_chrom1[0] - rand_chrom2[0]) / 100
    # difference_vec = np.subtract(rand_chrom1[1:], rand_chrom2[1:])
    # directed_vec = np.multiply(difference_vec, difference_accur)
    
    perturb_vec = average_chrom * random.random()
    # perturb_vec = average_chrom * avg_accur_diff
    directed_vec = np.subtract(max(good_solutions)[1:], perturb_vec)
    if increase_pop:
      new_chrom = np.add(random.choice(chroms_list)[1:], directed_vec)
    else:
      new_chrom = np.add(chroms_list[bad_chroms_ind[dsi]][1:], directed_vec)
    np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)

    new_res, new_chrom_accur = eval_chromosomes([new_chrom], num_chrom_params)
    if new_res[0] > best_res[0]:
      thresh_increase_rate += (new_res[0] - best_res[0])/5
      best_res = new_res
    
    if increase_pop:
      new_chromosomes.append(new_chrom_accur[0])
    else:
      if new_chrom_accur[0][0] > chroms_list[bad_chroms_ind[dsi]][0]:
        new_chromosomes.append(new_chrom_accur[0])

  return new_chromosomes


# -----------------------------------------------------------------------------
# local search functions

def local_search_v2(chrom1, chrom2, num_chrom_params):
  global best_res

  best_chrom = chrom1
  current_chrom = chrom1
  previous_chrom = chrom2

  max_iterations = 8 # max number of iterations to prevent algorithm being stuck in local search
  breakout_countdown = 8 - math.ceil(best_chrom[0]/20)

  lsi = 1
  reverse_direction = False
  while breakout_countdown >= 0 and max_iterations >= 0:

    print("=========================================================")
    print("local search iteration: {}".format(lsi+1))
    print("=========================================================")

    accuracy = current_chrom[0] / 100
    accuracy_diff = (current_chrom[0] - previous_chrom[0]) / 100
    if accuracy_diff < 0:
      reverse_direction = True
      accuracy_diff = abs(accuracy_diff)
    elif accuracy_diff <= 0.01 and accuracy_diff >= -0.01:
      # clip anything between 0% and 1% inclusive accuracy difference to 1%
      accuracy_diff = 0.01
    threshold_angle = math.pi/2 * (1 - accuracy_diff) * accuracy

    trajectory = np.subtract(current_chrom[1:], previous_chrom[1:])
    trajectory = trajectory * accuracy_diff
    rand_vec = np.random.rand(num_chrom_params) * 0.1
    rand_vec = rand_vec + trajectory

    # calculate the unit vector for original trajectory and the randomized vector
    # source: https://stackoverflow.com/a/2827475
    unit_vec_trajectory = trajectory / np.linalg.norm(trajectory)
    unit_vec_rand = rand_vec / np.linalg.norm(rand_vec)
    # calculate the angle between the two vectors
    angle_between_vec = np.arccos(np.clip(np.dot(unit_vec_trajectory, unit_vec_rand), -1.0, 1.0))

    if not reverse_direction:
      if angle_between_vec <= threshold_angle:
      # if their angle is within the threshold angle, then the randomized vector is accepted
        next_step = rand_vec
        # next_step = np.multiply(rand_vec, magnitude_ratio)
      else:
        # otherwise after some time, the default trajectory will be used
        next_step = trajectory
    else:
      if angle_between_vec > threshold_angle:
        # only if their angle is within the threshold angle, then the randomized vector is accepted
        next_step = rand_vec * -1
        # next_step = np.multiply(rand_vec, magnitude_ratio)
      else:
        # otherwise after some time, the default trajectory will be used
        trajectory = trajectory * -1
        next_step = trajectory
    
    new_chrom = current_chrom[1:] + next_step
    np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)
    
    new_res, new_chrom_accur = eval_chromosomes([new_chrom], num_chrom_params)
    if new_res[0] > best_res[0]:
      best_res = new_res

    # if the newly evaluated chromosome performs better than the currect best chromosome
    if new_chrom_accur[0][0] > best_chrom[0]:
      best_chrom = new_chrom_accur[0]
      breakout_countdown = 10 - math.ceil(best_chrom[0]/10)
    # apply some simulated annealing
    else:
      breakout_countdown -= 1
    
    max_iterations -= 1
    previous_chrom = current_chrom
    current_chrom = new_chrom_accur[0]
    trajectory = next_step
    reverse_direction = False
    lsi += 1

  return best_chrom


# -----------------------------------------------------------------------------
# final evaluation functions

def final_test(a_model):
  a_model.eval()
  test_loss = 0
  correct = 0
  test_preds = torch.zeros(0)
  first = True
  for test_data_in, test_data_out in test_loader: 

      if args['cuda']:
          test_data_in, test_data_out = test_data_in.cuda(), test_data_out.cuda()
              
      test_data_in, test_data_out = Variable(test_data_in), Variable(test_data_out)
      output = a_model(test_data_in)
      test_loss += F.nll_loss(output, test_data_out, reduction='sum').data # sum up batch loss

      pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
      if first:
        test_preds = pred
        first = False
      else:
        test_preds = torch.cat((test_preds, pred),0)

      correct += pred.eq(test_data_out.data.view_as(pred)).long().cpu().sum()

  # Print test accuracy
  test_loss /= len(test_loader.dataset)
  accuracy = 100. * correct / len(test_loader.dataset)
  print('\nTest set: Average loss: {:.4f}, Accuracy (at final epoch): {}/{} ({:.0f}%)\n'.format(
      test_loss, correct, len(test_loader.dataset), accuracy))

  return accuracy, test_loss


# -----------------------------------------------------------------------------------------------------------
# start of algorithm

starting_time = time.time() # starting time to keep track of the total time taken to complete
all_accur_valid = []  # data of validation accuracy of each trial is stored here
all_accur_final = []  # data of final accuracy of each trial is store here
all_average_loss = [[],[]] # data if average loss if both validation accuracy and final accuracy are stored here

# best_models_list = []
# best_chromosomes_list = []
# best_train_params_list = []

trials = 20 # 20
for ti in range(trials):
  meta_rs_valids = []
  best_model = None
  best_chromosome = None
  best_model_accur = 0

  do_eval_iter = 0
  local_search_iter = 0
  diff_search_iter = 0

  # list to store all initial search generated chromosomes
  mat_chroms = [[0 for x in range(comp_num_chrom_param(limits)+1)] for y in range(meta['max_rs_iter'])]


  # ---------------------------------------------------------------------------------------------------------
  # initial search

  # Start with a small search
  print('Initial random search ...')
  
  for rsi in range(meta['max_rs_iter']):

    if args['verbose_meta']:
      print('Search iteration {}.'.format(rsi+1))

    num_chrom_param = comp_num_chrom_param(limits)

    a_rand_chrom = gen_rand_chromosome(num_chrom_param)
    # a_rand_chrom = np.asarray([rsi/meta['max_rs_iter'] for i in range(num_chrom_param)])
    # if rsi == 0 or rsi == 1:
    #   a_rand_chrom = np.asarray([0.99999999999 for i in range(num_chrom_param)])
    # else:
    #   a_rand_chrom = np.asarray([rsi/meta['max_rs_iter'] for i in range(num_chrom_param)])

    # --- Actual training
    model, train_params = prepare_model(a_rand_chrom)
    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])
    best_valid_accur = max(valid_accurs)
    print('Best validation accuracy: {}%.'.format(best_valid_accur))

    # Store best model
    if best_valid_accur > best_model_accur:
      best_model_accur = best_valid_accur
      best_model = model
      best_train_params = train_params
      best_chromosome = a_rand_chrom

    meta_rs_valids.append(best_valid_accur)
    # adds the generated chromosome with its accuracy into the list
    mat_chroms[rsi][0] = best_valid_accur
    mat_chroms[rsi][1:] = a_rand_chrom
  
  print('*****************************************************{}'.format(ti))
  print('Best accuracy after initial random search: {}'.format(best_model_accur))
  print('*****************************************************')

  if args['verbose_meta']:
    print('Best validation errors:')
    print(meta_rs_valids)

  num_chrom_params = best_chromosome.shape[0]
  next_best_initial_chrom_ind = 1

  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)

  # -----------------------------------------------------------------------------------------------------------
  # population-based search

  # generate initial direction vector for each chromosome
  direction_vec = [np.random.rand(num_chrom_param)*0.2 for i in range(meta['max_rs_iter'])]
  direction_vec = [[i[j]-0.1 for j in range(len(direction_vec[0]))] for i in direction_vec]

  good_solutions = []
  bad_solutions = []
  good_sol_thresh = 2
  bad_sol_thresh = 15 # 5
  good_sol_radius = 6 # 11.5  # 6  # impact radius for good solutions
  bad_sol_radius = 6  # 11.5  # 6  # impact radius for bad solutions

  accuracy_thresh = 12.0

  sorted_mat_chroms = sorted(mat_chroms, reverse=True)
  good_solutions.append(sorted_mat_chroms[0])
  good_solutions.append(sorted_mat_chroms[1])
  for i, a_chrom in enumerate(mat_chroms):
    if a_chrom[0] <= accuracy_thresh:
      bad_solutions.append(a_chrom)
    else:
      if len(good_solutions) > good_sol_thresh:
        pass
      else:
        if a_chrom not in good_solutions:
          good_solutions.append(a_chrom)

  # sorted_mat_chroms = sorted(mat_chroms, reverse=True)
  # good_solutions.append(sorted_mat_chroms[0])
  # for i, a_chrom in enumerate(sorted_mat_chroms[1:]):
  #   if a_chrom[0] <= accuracy_thresh:
  #     if len(bad_solutions) > bad_sol_thresh:
  #       pass
  #     else:
  #       bad_solutions.append(a_chrom)
  #   else:
  #     if len(good_solutions) > good_sol_thresh:
  #       pass
  #     else:
  #       good_solutions.append(a_chrom)
  
  # bad_solutions.append(sorted_mat_chroms[len(sorted_mat_chroms)-1])

  # bad_solutions = sorted_mat_chroms[0:math.floor(len(sorted_mat_chroms)/2)]
  # good_solutions = sorted_mat_chroms[math.floor(len(sorted_mat_chroms)/2):]

  evaluated_chroms = []

  terminate_search = False
  pop_search_iter = 1

  has_good_sol = False
  no_good_sol_counter = 0
  while not terminate_search:
    thresh_increase_rate = 1.0
    print("========================================================={}".format(ti))
    print("population search iteration: {}".format(pop_search_iter))
    print("=========================================================")

    do_diff_search = False
    new_chromosomes = []
    for i, a_chrom in enumerate(mat_chroms):
      closest_dis = 19.4  # approximate Euclidean distance between two farthest points in the search space
      closest_good = False
      closest_bad = False

      # for j, sol in enumerate(good_solutions):
      #   if np.all(np.equal(a_chrom, sol)):
      #     continue
      #   # calculate Euclidean distance between the two chomosomes
      #   # source: https://stackoverflow.com/a/50639386
      #   euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
      #   # if the chromosome is close enough to a good solution, regardless of distance
      #   # set the good solution as the closest 
      #   if euclid_dis <= good_sol_radius and euclid_dis < closest_dis:
      #     closest_dis = euclid_dis
      #     closest_chrom = sol[1:]
      #     closest_good = True
      
      # # calculate Euclidean distance between current chromosome and good_sol / bad_sol
      # for j, sol in enumerate(bad_solutions):
      #   if closest_good:
      #     break
      #   if np.all(np.equal(a_chrom, sol)):
      #     continue
      #   # calculate Euclidean distance between the two chomosomes
      #   euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** .5
      #   # if the chromosome is within radius of a bad solution and the solution is the closest to it,
      #   # replace with new closest solution, otherwise ignore
      #   if euclid_dis <= bad_sol_radius and euclid_dis < closest_dis:
      #     closest_dis = euclid_dis
      #     closest_chrom = sol[1:]
      #     closest_bad = True
      
      # # used in order to get the gradient intersecting x=0 at 3 and x=radius at a value < 0.5, where radius is the good/bad solution radius
      # multiply_factor = 1 - closest_dis/20

      # if closest_good:
      #   # apply force to change direction of chromosome towards solution
      #   force_vec = np.subtract(a_chrom[1:], closest_chrom)
      #   force_vec = np.multiply(multiply_factor, force_vec)
      #   # inverse scaling to generate larger force the closer a chromosome is to a solution
      #   direction_vec[i] = np.add(direction_vec[i], force_vec)
      #   # np.clip(direction_vec[i], 0, 0.4, out=direction_vec[i])
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])
      # elif closest_bad:
      #   # apply force to change direction of chromosome away from solution
      #   force_vec = np.subtract(a_chrom[1:], closest_chrom)
      #   force_vec *= -1
      #   force_vec = np.multiply(multiply_factor, force_vec)
      #   # inverse scaling to generate larger force the closer a chromosome is to a solution
      #   direction_vec[i] = np.add(direction_vec[i], force_vec)
      #   # np.clip(direction_vec[i], 0, 0.4, out=direction_vec[i])
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])
      # else:
      #   # retain trajectory
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])

      # ------------------------------------------------------------------
      # CHANGES MADE
      # ------------------------------------------------------------------
      # all good and bad solutions will influence the current chromosome, not just the closest
      for j, sol in enumerate(good_solutions):
        if np.all(np.equal(a_chrom, sol)):
          continue
        # calculate Euclidean distance between the two chomosomes
        # source: https://stackoverflow.com/a/50639386
        euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
        diff_vec = np.subtract(sol[1:], a_chrom[1:])
        force_vec = np.multiply(diff_vec, 1 - euclid_dis/20)
        force_ratio = force_vec * (sol[0]/100)
        direction_vec[i] = np.add(direction_vec[i], force_ratio)
      
      # for j, sol in enumerate(bad_solutions):
      #   if np.all(np.equal(a_chrom, sol)):
      #     continue
      #   # calculate Euclidean distance between the two chomosomes
      #   # source: https://stackoverflow.com/a/50639386
      #   euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
      #   diff_vec = np.subtract(sol[1:], a_chrom[1:])
      #   diff_vec = diff_vec * -1
      #   force_vec = np.multiply(diff_vec, 1 - euclid_dis/20)
      #   force_ratio = force_vec * (sol[0]/100)
      #   direction_vec[i] = np.add(direction_vec[i], force_ratio)

      new_chrom = np.add(a_chrom[1:], direction_vec[i])
      np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)
      new_chromosomes.append(new_chrom)
    
    if len(new_chromosomes) <= 2 or no_good_sol_counter >= 3:
      new_chromosomes = [gen_rand_chromosome(num_chrom_param) for i in range(meta['min_pop_size']-1)]
      direction_vec = [(np.random.rand(num_chrom_param)*0.2)-0.1 for i in range(meta['min_pop_size']-1)]
      no_good_sol_counter = 0

    # test validation accuracy of new chromosomes
    new_res, evaluated_chroms = eval_chromosomes(new_chromosomes, num_chrom_params)
    if new_res[0] > best_res[0]:
      thresh_increase_rate += (new_res[0] - best_res[0])/5
      best_res = new_res
    
    good_chroms_list = []
    bad_chroms_ind = []
    for i, a_chrom in enumerate(evaluated_chroms):
      # if solution generated is worse or equal to the threshold
      if a_chrom[0] <= accuracy_thresh:
        bad_chroms_ind.append(i)

        if len(bad_solutions) >= bad_sol_thresh:
          bad_solutions[bad_solutions.index(max(bad_solutions))] = a_chrom.tolist()
        else:
          bad_solutions.append(a_chrom.tolist())
      else:
        has_good_sol = True
        # perform local search when is a potentially good solution
        new_chrom = local_search_v2(a_chrom, max(good_solutions), num_chrom_params)
        good_chroms_list.append(new_chrom)

        if len(good_solutions) >= good_sol_thresh:
          good_solutions[good_solutions.index(min(good_solutions))] = new_chrom.tolist()
        else:
          good_solutions.append(new_chrom.tolist())
      
      accuracy_thresh += thresh_increase_rate
      thresh_increase_rate = 0.0
    
    if not has_good_sol:
      # increase good solution radius, decrease bad solution radius
      good_sol_radius /= 0.9
      bad_sol_radius *= 0.9
      no_good_sol_counter += 1
    else:
      # reset radius
      good_sol_radius = 6
      bad_sol_radius = 6
    
    good_chroms = []
    for i in evaluated_chroms:
      good_chroms.append(i.tolist())
    good_chroms = sorted(good_chroms, reverse=True)[0:len(good_chroms_list)]
    new_direction_vec = []
    for i in good_chroms:
      new_direction_vec.append(direction_vec[evaluated_chroms.tolist().index(i)])
    direction_vec = new_direction_vec

    # if population is below a certain number of chroms, add the new chromosomes into the population
    if len(mat_chroms) < meta['min_pop_size']:
      increase_pop = True
    else:
      increase_pop = False
    diff_chromosomes = differential_search_v3(evaluated_chroms, bad_chroms_ind, good_solutions, bad_solutions, increase_pop, num_chrom_params)
    # diff_chromosomes = differential_search_v4(evaluated_chroms, bad_chroms_ind, good_solutions, increase_pop, num_chrom_params)

    mat_chroms = good_chroms_list + diff_chromosomes
    for i in range(len(diff_chromosomes)):
      direction_vec.append((np.random.rand(num_chrom_param)*0.2)-0.1)
    
    has_good_sol = False
    thresh_increase_rate += 1

    print('======================================{}'.format(ti))
    print('Best accuracy so far: {}'.format(best_res[0]))
    print('======================================')

    pop_search_iter += 1
      

  # -----------------------------------------------------------------------------------------------------------
  # final evaluation

  # ==========================
  # Test final model / Visualize predictions
  # ==========================

  # Final visualization
  # print('======================================')
  # print('Best accuracy so far: {}'.format(best_res[0]))
  # print('======================================')
  print('Computing the final test ...')
  
  best_model_accur, best_chromosome, best_model, best_training_params = best_res
  best_final_model, best_validation_model, best_valid_accurs, best_train_accurs = do_training(best_model, best_training_params, args['num_epochs_test'])

  print('=====================================')
  print('Training accuracies')
  print('=====================================')
  print(best_train_accurs)
  print('=====================================')
  print('Best training parameters')
  print('=====================================')
  print(best_training_params)
  print('=====================================')
  print('Best model')
  print('=====================================')
  print(best_validation_model)
  print('=====================================')
  print('Test accuracies')
  print('=====================================')
  print('Model with best validation accuracy: ')
  accur_valid, test_loss = final_test(best_validation_model)
  all_average_loss[0].append(test_loss.item())
  print('Model at the end of training: ')
  accur_final, test_loss = final_test(best_final_model)
  all_average_loss[1].append(test_loss.item())

  # max_accur = int(max(accur_valid, accur_final))
  # print("The max accuracy is: {}".format(max_accur))

  # best_models_list.append(best_validation_model)
  # best_chromosomes_list.append(best_chromosome)
  # best_train_params_list.append(best_training_params)

  all_accur_valid.append(accur_valid)
  all_accur_final.append(accur_final)
    
valid_accuracies = [i.item() for i in all_accur_valid]
final_accuracies = [i.item() for i in all_accur_final]
print("All valid accuracies: {}".format(valid_accuracies))
print("All final accuracies: {}".format(final_accuracies))
print("All average validation loss: {}".format(all_average_loss[0]))

print("Mean accuracy:         {}".format(np.mean(valid_accuracies)))
print("Standard deviation:    {}".format(np.std(valid_accuracies)))
print("Minimum accuracy:      {}".format(min(valid_accuracies)))
print("1st Quartile accuracy: {}".format(np.percentile(valid_accuracies, 25)))
print("Median accuracy:       {}".format(np.median(valid_accuracies)))
print("3rd Quartile accuracy: {}".format(np.percentile(valid_accuracies, 75)))
print("Maximum accuracy:      {}".format(max(valid_accuracies)))
print("Interquartile range:   {}".format(np.percentile(valid_accuracies, 75) - np.percentile(valid_accuracies, 25)))
print("Total time taken: {}".format((time.time()-starting_time)))

print("Do eval calls per trial: {}".format(do_eval_iter))
# print("Take previous: {}".format(testing['take_previous']))
# print("Take random: {}".format(testing['take_random']))
# print("All best models: {}".format(best_models_list))
# print("All best chromosomes: {}".format(best_chromosomes_list))
# print("All best training parameters: {}".format(best_train_params_list))
# print("Population search iterations per trial: {}".format(pop_search_iter))
# print("Local search iterations per trial: {}".format(local_search_iter))
# print("Differential search iterations per trial: {}".format(diff_search_iter))

# --- Saving the best chromosome as a csv file
if args['save_best_chrom']:

  print('Saving the best chromosome in the root directory ...')

  from datetime import datetime
  from numpy import asarray
  from numpy import savetxt
  import numpy as np

  def get_date_time_str():
    now = datetime.now() # current date and time
    return now.strftime("%d%m%Y_%H%M")

  # Create filename
  path = './'
  filename = get_date_time_str() + "_" + str(max_accur)
  savetxt(path+filename, best_chromosome, delimiter=',')

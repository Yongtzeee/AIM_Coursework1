# -*- coding: utf-8 -*-
"""aim_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ERM7POn4ZvRlPESlR8j774HaLRdKKggr
"""

# customized code
# -*- coding: utf-8 -*-
"""gbndm_aim_0_3_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ny3YOI8G7xTqCFgCdqpdkZvI6yrfwdIe
"""

#version = '_v0p3p9_'

from __future__ import print_function
import argparse
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import time
import warnings

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab

from sklearn import cluster, datasets, mixture
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler
from itertools import cycle, islice

import torch
import numpy as np
from torch.utils import data
import copy

from torchvision import datasets, transforms
from torch.autograd import Variable

from IPython.core.debugger import set_trace

# ===============
# Initializations
# ===============

#!pip install torch torchvision

# -----------------------------------------------------------------------------------------------------------

# ===============
# Parameters
# ===============

# --- General framework arguments

args = {}
kwargs = {}
args['num_train_batch'] = 1  # number of MNIST training batches
args['num_valid_batch'] = 10  # number of MNIST validation batches
args['train_batch_size'] = 100   # training batch size
args['valid_batch_size'] = 100   # validation batch size
args['test_batch_size'] = 10
# args['epochs'] = 100  # number of training epochs 
args['lr'] = 1 # 0.1  # 0.01 # learning rate # this is over-written by the solution
args['momentum'] = 0.5 # SGD momentum (default: 0.5); momentum is a moving average of our gradients (helps keep a useful direction)
args['clip_level'] = 0.5  # gradient clip threshold
args['seed']= 1 #random seed
args['log_interval_epoch'] = 1 # display training loss every log_int... epochs
args['cuda'] = True 
args['patience'] = 1000  # stop train. if the valid. err. hasn't improved by this num. of epochs
args['noise_in'] = -1   # 0.5  # 0.15  # amount of noise to add to the training data
args['noise_out'] = -1   #  probability of changing an output label to some random label
args['verbose_train'] = False   # print status of model training?
args['verbose_meta'] = True #  print status of architecture optimization?
args['min_inst_class'] = 5 # minimum number of instances per class in the training set
args['batch_max_tries'] = 10 # max. num. of attempts in extracting data batches
args['save_best_chrom'] = False # False  # save the best chromose in Google Drive?
args['dc_prob_drop'] = 0.5 # 0.5 # probability of dropping a circuit (DropCircuit)
args['prob_sel_branch'] = 0.5 # probability of architectural search selecting/using a branch (this is not DropCircuit) 

# --- Neural architectural limits

args['num_epochs_search'] = 10 # 100  # number of epochs for training during architecture search 
args['num_epochs_test'] = 1000  # num. epochs for training during the final test
limits = {}
limits['min_layer_nodes'] =  10 # 5 # 50
limits['max_layer_nodes'] = 100
limits['max_pre_branch_layers'] = 5 # 3
limits['max_branches'] = 10
limits['max_branch_layers'] = 5  # 3
limits['max_post_branch_layers'] = 5 # 3
limits['lr1_min'] = 0.001
limits['lr1_,max'] = 2
limits['lr2_min'] = 0.001
limits['lr2_,max'] = 2
limits['moment_min'] = 0.001
limits['moment_max'] = 1

# np.random.seed(0)
# torch.manual_seed(0)

num_train_instances = args['num_train_batch'] * args['train_batch_size']
num_valid_instances = args['num_valid_batch'] * args['valid_batch_size']

data_rand_seed = 1 # (other pre-tested seeds: 2, 3)
do_eval_iter = 0

# -----------------------------------------------------------------------------------------------------------

# ============
# Load dataset
# ============

# Seed creation
torch.manual_seed(data_rand_seed)
np.random.seed(data_rand_seed)

a_data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
  
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                   transform=a_data_transform),
    batch_size=args['train_batch_size'], shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=a_data_transform),
    batch_size=args['test_batch_size'], shuffle=True, **kwargs)

# Check whether we have enough instances per class
# We want an imbalanced dataset but we don't want any specific label having
# an "insufficient" number of instances.
def check_enough_inst(batches, min_inst_per_class):
  
  # Concatenate training batch output labels
  labels = batches[0][1].numpy()
  for batch_i in range(1,args['num_train_batch']):
    new_labels = batches[batch_i][1].numpy()
    labels = np.concatenate((labels, new_labels))

  labels = labels.tolist()

  # Scan though labels
  for a_label in range(10):  # assuming MNIST, of course
    a_count = labels.count(a_label)
    # If label count is < min_inst_per_class return False
    if a_count < min_inst_per_class:
      return False

  # Return True
  return True


# Prepare data. Extract training and validation batches
# This is where we make the problem "small and imbalanced"
def extract_batches(a_loader, min_inst_per_class, max_tries):

  # Keep trying until you have a required minimum number of instances 
  # per class in the training set (not elegant but ok for the range of 
  # "min_inst_per_class" we need)

  for ti in range(max_tries):

    print('Data extraction trial {}.'.format(ti))

    # Initializations
    train_batches = []
    valid_batches = []
    tot_batch_extract = args['num_train_batch'] + args['num_valid_batch']

    # Extract
    for batch_idx, (data, target) in enumerate(a_loader):

      if batch_idx < args['num_train_batch']:
        train_batches.append((data,target))
      else:
        valid_batches.append((data,target))
        if batch_idx == tot_batch_extract - 1:
          break

    # Check minimum number of instances
    enough_instaces = check_enough_inst(train_batches, min_inst_per_class)
    if enough_instaces:
      return train_batches, valid_batches

  print('It was not possible to create a dataset.')
  print('Consider increasing the overall number of instances, or')
  print('decreasing the minimum instances per class allowed.')
  return [], []


train_batches, valid_batches = extract_batches(train_loader, args['min_inst_class'], args['batch_max_tries'])
if args['verbose_train']:
  print('Extracted {} train_batches, and {} valid_batches.'.format(len(train_batches), len(valid_batches)))

# -----------------------------------------------------------------------------------------------------------

# ===========================
# Display histogram of labels
# ===========================

def disp_hist_labaels(batches):
  # Concatenate training batch output labels
  labels = batches[0][1].numpy()		# batches[0][0] are the pictures, batches[0][1] are labels for each picture
  for batch_i in range(1,args['num_train_batch']):
    new_labels = batches[batch_i][1].numpy()
    labels = np.concatenate((labels, new_labels))

  num_bins = 10
  n, bins, patches = plt.hist(labels, num_bins, facecolor='blue', alpha=0.5)
  plt.show()

disp_hist_labaels(train_batches)

# ==========================
# Display dataset
# ==========================

# This is not currently used

# Display MNIST instances
# Adapted from # https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb
def disp_MNIST_inst(num_disp, X_train, y_train):
  pltsize=1
  plt.figure(figsize=(num_disp*pltsize, pltsize))
  for i in range(num_disp):
    plt.subplot(1,num_disp,i+1)
    plt.axis('off')
    plt.imshow(X_train[i,:,:].numpy().reshape(28,28), cmap="gray_r")
    plt.title('Class: '+str(y_train[i].item()))

if args['verbose_train']:
  X_train = train_batches[0][0]
  y_train = train_batches[0][1]

  disp_MNIST_inst(10, X_train, y_train)

  sum_train_0 = X_train[0,:,:].sum()
  min_train_0 = X_train[0,:,:].min()
  max_train_0 = X_train[0,:,:].max()

  print('X_train[0,:,:] --> sum ({}); min ({}); max ({}).'.format(sum_train_0, min_train_0, max_train_0))

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Design model
# ==========================

# "Unseed" the rest
np.random.seed()
a_rand_seed = np.random.randint(0,999999)
torch.manual_seed(a_rand_seed)

# Simple function to scale parameters
# num is assume to be \in [0,1]
def scale_to_range(num,min_val,max_val):
  range = max_val - min_val
  return (num*range)+min_val

# Simple layer for doing elementwise scaling
# Adapated from https://stackoverflow.com/questions/51980654/pytorch-element-wise-filter-layer
class Elem_Scaling_1D(nn.Module):
  def __init__(self, num_nodes, bogus):  # clean-up "bogus"
    super(Elem_Scaling_1D, self).__init__()
    # Initialize
    init_weights = np.random.normal(loc=0, scale=0.5, size=np.shape(num_nodes))
    # Assign
    self.weights = torch.from_numpy(init_weights)
    #self.weights = nn.Parameter(torch.Tensor(1, num_nodes))  # trainable parameter

  def forward(self, x):
    # assuming x is of size num_inst-1-num_nodes
    return x * self.weights  # element-wise multiplication

# Class gradient-based neural diversity machine
class GBNDM(nn.Module):
    def __init__(self, a_chromosome):   # assuming MNIST
        super(GBNDM, self).__init__()

        # --------- Pre-branch layers 
        self.chromosome = a_chromosome
        self.pre_branch_layers = nn.ModuleList()
        prev_num_out = 28*28
        chrom_ind = 5 # skip over 3 learning rate param., 1 moment. p. (interp./used in training)
        # and 1 exist-or-not (may use in future versions).
        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
        for i in range(limits['max_pre_branch_layers']):
          
          # Extract and interpret parameters
          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
          layer_params_real = self.interp_layer_param(layer_params_raw)
        
          # Decide on whether to create a layer or not
          if i==0:  # the first layer of each segment is done by default
            do_layer = True
          else:
            do_layer = layer_params_real[0]

          if do_layer:
            # Create layer    
            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
            # Append layer and update chromosome index
            self.pre_branch_layers.append(a_layer)
            
          chrom_ind += 6

        # --------- Branches
        num_out_pre_branch = prev_num_out

        self.branches = nn.ModuleList()

        # Scan over branches
        final_num_nodes = []  # keep track of the size of the final layer of each branch
        count_branches = 0
        for bi in range(limits['max_branches']):

          # Initialize branch
          branch_layers = nn.ModuleList()

          # Do branch? Always do the first one by default
          if (a_chromosome[chrom_ind] < args['prob_sel_branch']) or (bi == 0):
            do_branch = True
          else: 
            do_branch = False
            
          chrom_ind += 1

          # Scan over branch layers

          if do_branch:  # if doing branch

            this_fin_num_nodes = 0

            count_branches += 1
            
            for li in range(limits['max_branch_layers']):

              # Extract and interpret parameters
              layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
              layer_params_real = self.interp_layer_param(layer_params_raw)

              # Decide on whether to create a layer or not
              if li==0:  # the first layer of each branch is done by default
                do_layer = True
                prev_num_out = num_out_pre_branch
              else:
                do_layer = layer_params_real[0]

              if do_layer:
                # print('... temp ... this_fin_num_nodes: {}.'.format(this_fin_num_nodes))
                # Create layer    
                a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
                # Num_nodes - keep storing the latest one; the last latest is the final layer num_nodes
                this_fin_num_nodes = prev_num_out
                # Append layer and update chromosome index
                branch_layers.append(a_layer)

              chrom_ind += 6

            # Append branch
            final_num_nodes.append(this_fin_num_nodes)
            #print('final_num_nodes: {}.'.format(final_num_nodes))
            self.branches.append(branch_layers)

          else: # if not doing branch you still need to increment chromosome index

            chrom_ind += 6*limits['max_branch_layers']

        self.num_branches = count_branches
        self.dc_prob_activ = 1 - args['dc_prob_drop'] # probability of using a circuit
        self.tot_nodes_branch_final = sum(final_num_nodes)
        
        # --------- Post-branch layers

        prev_num_out = self.tot_nodes_branch_final
        
        # --- Create post-branch layers

        chrom_ind += 1  # skip over the parameter pertaining to the existence or not of the post-branch segment

        self.post_branch_layers = nn.ModuleList()
        
        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
        for li in range(limits['max_post_branch_layers']):
          
          # Extract and interpret parameters
          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]
          layer_params_real = self.interp_layer_param(layer_params_raw)
        
          # Decide on whether to create a layer or not
          if li==0:  # the first layer of each segment is done by default
            do_layer = True
          else:
            do_layer = layer_params_real[0]

          if do_layer:
            # Create layer    
            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)
            # Append layer and update chromosome index
            self.post_branch_layers.append(a_layer)
            
          chrom_ind += 6

        # Create a final layer
        self.final_layer = nn.Linear(prev_num_out, 10)

    def forward(self, x, dc_mask = None):
        
        x = x.view(-1, 28*28)

        # Apply pre-branch layers
        for pi, a_layer in enumerate(self.pre_branch_layers):
          #print('Pre-layer {}'.format(pi))
          # if isinstance(a_layer, Elem_Scaling_1D):
          #   set_trace()
          x = a_layer(x)

        # Apply branches
        branch_finals = []
        for bi, a_branch in enumerate(self.branches):
          #print('Branch {}'.format(bi))
          
          z = x

          for a_layer in a_branch:
            # if isinstance(a_layer, Elem_Scaling_1D):
            #   set_trace()
            z = a_layer(z)
          
          if self.training:
            z = (dc_mask[bi] * z) / self.dc_prob_activ  # if training apply DropCircuit
          
          branch_finals.append(z)

        # Concatenate multi-branch final layers
        x = torch.cat(branch_finals,dim=1)

        # if not(self.training): # if not training then testing/evaluating
        #   x = self.dc_prob_activ * x  # scaling due to DropCircuit
      

        # Apply post-branch layers
        for pi, a_layer in enumerate(self.post_branch_layers):
          #print('Post-layer: {}'.format(pi))
          # if isinstance(a_layer, Elem_Scaling_1D):
          #   set_trace()
          x = a_layer(x)

        # Apply final layer
        x = self.final_layer(x)
               
        return F.log_softmax(x, dim=1)


    # Method for interpreting layer parameters
    # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]
    def interp_layer_param(self, layer_params_raw):
      # Exist or not
      if layer_params_raw[0] < 0.5:
        exist = False
      else:
        exist = True
      # Weight function
      tot_weight_func = 3
      if layer_params_raw[1] < 0.6: # or ... (1/tot_weight_func):  
        weight_func_sel = 'linear'
      elif layer_params_raw[1] < 0.8: # or ... (2/tot_weight_func):
        weight_func_sel = 'elem_scale'
      else:
        weight_func_sel = 'id'
      # activation function
      tot_node_func = 22
      if layer_params_raw[2] < (1/tot_node_func):
        activ_func = 'ReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (2/tot_node_func):
        activ_func = 'Hardshrink'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (3/tot_node_func):
        activ_func = 'Hardtanh'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,2)
        if param_1 > param_2:  # param_1 is min_val; param_2 is max_val
          tmp_val = param_1
          param_1 = param_2
          param_2 = tmp_val
        elif param_1 == param_2:
          param_2 += 0.1
      elif layer_params_raw[2] < (4/tot_node_func):
        activ_func = 'LeakyReLU'
        param_1 = scale_to_range(layer_params_raw[3],0,1)
        param_2 = -1
      elif layer_params_raw[2] < (5/tot_node_func):
        activ_func = 'LogSigmoid'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (6/tot_node_func):
        activ_func = 'PReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (7/tot_node_func):
        activ_func = 'ELU'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (8/tot_node_func):
        activ_func = 'ReLU6'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (9/tot_node_func):
        activ_func = 'RReLU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (10/tot_node_func):
        activ_func = 'SELU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (11/tot_node_func):
        activ_func = 'CELU'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (12/tot_node_func):
        activ_func = 'GELU'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (13/tot_node_func):
        activ_func = 'Sigmoid'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (14/tot_node_func):
        activ_func = 'Softplus'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,40)
      elif layer_params_raw[2] < (15/tot_node_func):
        activ_func = 'Softshrink'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = -1
      elif layer_params_raw[2] < (16/tot_node_func):
        activ_func = 'Softsign'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (17/tot_node_func):
        activ_func = 'Tanh'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (18/tot_node_func):
        activ_func = 'Tanhshrink'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (19/tot_node_func):
        activ_func = 'Threshold'
        param_1 = scale_to_range(layer_params_raw[3],0,2)
        param_2 = scale_to_range(layer_params_raw[4],0,2)
      elif layer_params_raw[2] < (20/tot_node_func):
        activ_func = 'Softmin'
        param_1 = -1
        param_2 = -1
      elif layer_params_raw[2] < (21/tot_node_func):
        activ_func = 'Softmax'
        param_1 = -1
        param_2 = -1
      else: 
        activ_func = 'None'
        param_1 = -1
        param_2 = -1

      # number of nodes
      num_nodes = scale_to_range(layer_params_raw[5], limits['min_layer_nodes'], limits['max_layer_nodes'])
      num_nodes = num_nodes.astype(int)

      return (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)

    # Method for creating a layer
    # layer_params_real format: (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)
    def create_layer(self, prev_num_out, layer_params_real):
      
      exist, weight_func_sel, activ_func, param_1, param_2, num_nodes = layer_params_real
      
      # wf_param1/wf_param2 --> not elegant 
      if weight_func_sel == 'linear':
        weight_func = nn.Linear
        num_nodes_in = prev_num_out
        num_nodes_out = num_nodes
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
      elif weight_func_sel == 'id':
        weight_func = nn.Identity
        num_nodes_in = prev_num_out
        num_nodes_out = prev_num_out
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
      else:  # 'elem_scale'
        weight_func = Elem_Scaling_1D
        num_nodes_in = prev_num_out
        num_nodes_out = prev_num_out
        wf_param1 = num_nodes_in
        wf_param2 = num_nodes_out
        
      if activ_func == 'ReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU())
      elif activ_func == 'Hardshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Hardshrink(param_1))
      elif activ_func == 'Hardtanh':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Hardtanh(param_1, param_2))
      elif activ_func == 'LeakyReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.LeakyReLU(param_1))
      elif activ_func == 'LogSigmoid':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.LogSigmoid())
      elif activ_func == 'PReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.PReLU())
      elif activ_func == 'ELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ELU(param_1))
      elif activ_func == 'ReLU6':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU6())
      elif activ_func == 'RReLU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.RReLU())
      elif activ_func == 'SELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.SELU())
      elif activ_func == 'CELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.CELU(param_1))
      elif activ_func == 'GELU':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.ReLU())  # for some reason GELU is not present; revert later if relevant
      elif activ_func == 'Sigmoid':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Sigmoid())
      elif activ_func == 'Softplus':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softplus(param_1, param_2))
      elif activ_func == 'Softshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softplus(param_1))
      elif activ_func == 'Softsign':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softsign())
      elif activ_func == 'Tanh':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Tanh())
      elif activ_func == 'Tanhshrink':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Tanhshrink())
      elif activ_func == 'Threshold':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Threshold(param_1, param_2))
      elif activ_func == 'Softmin':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softmin())
      elif activ_func == 'Softmax':
        a_layer = nn.Sequential(
            weight_func(wf_param1, wf_param2),
            nn.Softmax())
      else:
        a_layer = weight_func(wf_param1, wf_param2)

      return a_layer, num_nodes_out

# -----------------------------------------------------------------------------------------------------------
# Generating chromosomes

# Generate a random chromosome where each param. is \in [0,1)
def gen_rand_chromosome(num_param):
  chrom = np.random.rand(num_param)
  return chrom

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Train and test functions
# ==========================

# Create a random mask for DropCircuit
def make_mask(num_circuits, prob_drop):
  rand_vals = np.random.rand(num_circuits)
  decisions = rand_vals >= prob_drop
  a_mask = np.ones(num_circuits)*decisions
  # Must have at least one circuit active
  if np.sum(a_mask) == 0:
    rand_index = np.random.randint(num_circuits)
    a_mask[rand_index] = 1.0

  return torch.from_numpy(a_mask)

def train_one_epoch(a_model, optimizer, epoch, batches):

  a_model.train()
  for batch_idx, (data, target) in enumerate(batches):
      if args['cuda']:
          data, target = data.cuda(), target.cuda()
      # Variables in Pytorch are differentiable. 
      
      data, target = Variable(data), Variable(target)
      #This will zero out the gradients for this batch. 
      optimizer.zero_grad()
      
      # Create DropCircuit mask
      dc_mask = make_mask(a_model.num_branches, args['dc_prob_drop'])
      
      output = a_model(data, dc_mask)

      # Calculate the negative log likelihood loss.
      loss = F.nll_loss(output, target)
      #dloss/dx for every Variable 
      loss.backward()
      # Gradient clipping
      torch.nn.utils.clip_grad_norm_(a_model.parameters(), args['clip_level'])
      #to do a one-step update on our parameter.
      optimizer.step()
      #Print out the loss periodically. 

  if args['verbose_train']:
    if epoch % args['log_interval_epoch'] == 0:
      print('Epoch: {}. Latest loss: {}.'.format(epoch, loss.data))

# Compute accuracy
# The argument (data_source) of this function can be a data_loader or a list of batches (previously extracted)
def comp_accuracy(a_model, data_source, src_num_instances):
    a_model.eval()
    a_loss = 0
    correct = 0
    preds = torch.zeros(0)
    first = True
    for a_data_in, a_data_out in data_source:
        if args['cuda']:
            a_data_in, a_data_out = a_data_in.cuda(), a_data_out.cuda()
        a_data_in, a_data_out = Variable(a_data_in), Variable(a_data_out)
        output = a_model(a_data_in)

        a_loss += F.nll_loss(output, a_data_out, reduction='sum').data # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        if first:
          preds = pred
          first = False
        else:
          preds = torch.cat((preds, pred),0)

        correct += pred.eq(a_data_out.data.view_as(pred)).long().cpu().sum()

    # Compute and return accuracy
    if type(data_source) == list:  # case: list of batches
      result = 100. * (correct.numpy()/ src_num_instances)
    else: # case: data_loader
      result = 100. * (correct.numpy()/ len(data_source.dataset))
    return result

# -----------------------------------------------------------------------------------------------------------

# ==========================
# Evaluate and Train functions
# ==========================

# Compute the number of parameters in a chromose (depends on limits)
def comp_num_chrom_param(limits):
  # Num. of training parameters
  tot_train_param = 4 # lr1, lr2, lr2 prop, momentum
  # Num. of pre-branch parameters
  tot_pre_branch = 1+(6*limits['max_pre_branch_layers']) # exist-or-not, layer params.
  # Num. of branch parameters
  tot_branch = (1+(6*limits['max_branch_layers']))*limits['max_branches']
  # Num. of post-branch parameters
  tot_post_branch = 1+(6*limits['max_post_branch_layers'])

  return tot_train_param+tot_pre_branch+tot_branch+tot_post_branch

# Interpret learning rate and momentum parameters
def interp_lrm(params):
  lr1 = scale_to_range(params[0], limits['lr1_min'], limits['lr1_,max'])
  lr2 = scale_to_range(params[1], limits['lr2_min'], limits['lr2_,max'])
  if lr1 < lr2:
    temp = lr1
    lr1 = lr2
    lr2 = temp
  lr2_epoch = (np.round(params[2]*args['num_epochs_search'])).astype(int)
  a_decr = (lr1-lr2)/lr2_epoch
  a_momentum = scale_to_range(params[3], limits['moment_min'], limits['moment_max'])
  return lr1, lr2, lr2_epoch, a_momentum, a_decr

# Chromosome evaluation (more efficient than do_training)
# Function to train a specific model
# Early stopping, or returning best validation model, is not implemented 
def do_eval_chrom(a_model, train_params, num_epochs):
  global do_eval_iter
  do_eval_iter += 1

  # Extract basic information
  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  
  args['lr'] = lr1
  args['momentum'] = a_momentum
  
  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])
  valid_accurs = []
  train_accurs = []
  train_start_time = time.time()
  for epoch in range(1, num_epochs + 1):

    if args['verbose_train']:
      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))

    train_one_epoch(a_model, optimizer, epoch, train_batches)
    
    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)
    if args['verbose_train']:
      print('Training accuracy: {}%.'.format(a_train_accur))
    train_accurs.append(a_train_accur)
    
    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)
    if args['verbose_train']:
      print('Validation accuracy: {}%.'.format(a_valid_accur))
    valid_accurs.append(a_valid_accur)

    # Decrement learning rate
    if epoch < lr2_epoch:
        args['lr'] -= lr_decr
        for param_group in optimizer.param_groups:
          param_group['lr'] = args['lr']

  train_elapsed_time = time.time() - train_start_time
  print('The training process took {} seconds.'.format(train_elapsed_time))

  return a_model, valid_accurs, train_accurs

# Function to train a specific model
# Early stopping, or returning best validation model, is not implemented 
def do_training(a_model, train_params, num_epochs):

  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  
  args['lr'] = lr1
  args['momentum'] = a_momentum

  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])
  best_valid_accur = 0
  best_model = type(a_model)(a_model.chromosome) # get a new instance
  valid_accurs = []
  train_accurs = []
  train_start_time = time.time()
  for epoch in range(1, num_epochs + 1):

    if args['verbose_train']:
      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))

    train_one_epoch(a_model, optimizer, epoch, train_batches)
    
    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)
    if args['verbose_train']:
      print('Training accuracy: {}%.'.format(a_train_accur))
    train_accurs.append(a_train_accur)
    
    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)
    if args['verbose_train']:
      print('Validation accuracy: {}%.'.format(a_valid_accur))
    valid_accurs.append(a_valid_accur)
    if a_valid_accur > best_valid_accur:
      best_valid_accur = a_valid_accur
      # best_model.load_state_dict(a_model.state_dict()) # copy weights and stuff
      best_model = copy.deepcopy(a_model)

    # Decrement learning rate
    if epoch < lr2_epoch:
        args['lr'] -= lr_decr
        for param_group in optimizer.param_groups:
          param_group['lr'] = args['lr']

  train_elapsed_time = time.time() - train_start_time
  print('The training process took {} seconds.'.format(train_elapsed_time))

  if args['cuda']:
      best_model.cuda()

  return a_model, best_model, valid_accurs, train_accurs

# -----------------------------------------------------------------------------------------------------------

# ================================
# Architectural search
# ================================

# Artificial Intelligence Methods students should focus on the code below.
# You should keep the neural network code unchanged. This is crucial for 
# comparison purposes. In other words, focus only on modifying the architecture
# search code below.

import random
import math

# --- Architectural search parameters
meta = {}
meta['max_rs_iter'] = 6 # 10  # initial random search
meta['max_shc_iter'] = 20 # 40 # 20 # 40  # stochastic hill climbing iterations
meta['num_differential_sol'] = 4 # 8 # number of differential evolution solutions
meta['diff_lr'] = 0.4 # 0.5 # 0.1 # learning rate for differential search
meta['num_neighbours'] = 8 # 16 
meta['neighbour_range'] = 0.2  # 0.4  # mutation rate for stochastic hill-climbing

# --- Population based search parameters (genetic algorithm)
# meta['population_search_iter'] = 6  # population based genetic algorithm search iteration
meta['mutation_rate'] = 0.2 # mutation rate of a newly generated chromosome

meta['local_search_iter'] = 4 # number of local search iterations
meta['step_size'] = 0.1 # step size for local search

# testing parameters
testing = {}
testing['take_previous'] = 0
testing['take_random'] = 0
testing['limit_numbers'] = [0 for i in range(10)]

# -----------------------------------------------------------------------------
# preparations functions

# Prepare model
def prepare_model(a_rand_chrom):

  # Initialize chromosome and model
  model = GBNDM(a_rand_chrom)
  
  if args['cuda']:
      model.cuda()

  # Interpret learning rates and momentum
  lr1, lr2, lr2_epoch, a_momentum, lr_decr = interp_lrm(a_rand_chrom[0:4])
  train_params = (lr1, lr2, lr2_epoch, a_momentum, lr_decr)
  args['momentum'] = a_momentum
  if args['verbose_train']:
    print('lr1: {}'.format(lr1))
    print('lr2: {}'.format(lr2))
    print('lr2_epoch: {}'.format(lr2_epoch))
    print('a_momentum: {}'.format(a_momentum))
    print('a_decr: {}'.format(a_decr))

  return model, train_params


# -----------------------------------------------------------------------------
# stochastic hill-climbing algorithm functions

# Evaluate a list of chromosomes
def eval_chromosomes(list_chromosomes,num_chrom_params):
  global do_eval_iter, terminate_search
  
  # best_model_accur, best_chromosome, best_model, best_train_params = best_res
  
  best_model_accur = 0
  best_chromosome = None
  best_model = None
  best_train_params = None

  num_chrom = len(list_chromosomes)
  mat_chrom_acur = np.zeros((num_chrom, 1+num_chrom_params))
  neighb_valid_accurs = []

  for ci, a_chrom in enumerate(list_chromosomes):
    if do_eval_iter >= 250:
      terminate_search = True
      break
    if args['verbose_meta']:
      print('Chromosome {} ...'.format(ci))
    # --- Actual training
    model, train_params = prepare_model(a_chrom)
    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])
    best_valid_accur = max(valid_accurs)
    # Store chromosome and accuracy
    mat_chrom_acur[ci,0] = best_valid_accur
    mat_chrom_acur[ci,1:] = a_chrom
    print('Best validation accuracy: {}%.'.format(best_valid_accur))
    neighb_valid_accurs.append(best_valid_accur)
    if best_valid_accur > best_model_accur:
      best_model_accur = best_valid_accur
      best_chromosome = a_chrom
      best_model = model
      best_train_params = train_params
      if args['verbose_meta']:
        print('*** Improving validation accuracy: {}.'.format(best_model_accur))
  
  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)
  return best_res, mat_chrom_acur


# -----------------------------------------------------------------------------
# differential search functions

# differential function focused on perturbation

# Simple differential search v3
def do_diff_chrom_v3(mat_chrom_accur, num_new_sol, num_chrom_params):
  global best_res, diff_search_iter
  
  # Initialize new chromosomes
  num_chrom = mat_chrom_accur.shape[0]
  if num_new_sol >= num_chrom:
    num_new_sol = num_chrom-1
  new_chromosomes = []
  new_chromosomes.append(mat_chrom_accur[0].tolist())

  # Sort the array of chromosomes based on the first column (contains accur.)
  mat_chrom_accur = mat_chrom_accur[(-mat_chrom_accur[:,0]).argsort()]
  # Extract first/best chromosome
  best_chrom = mat_chrom_accur[0,1:]
  best_accur = mat_chrom_accur[0][0] / 100
  worst_accur = mat_chrom_accur[len(mat_chrom_accur)-1][0] / 100

  for dsi in range(1, len(mat_chrom_accur)):

    print("=========================================================")
    print("differential search iteration: {}".format(dsi))
    print("=========================================================")

    # 1. calculate difference of chromosomes params between best and ith chromosome
    ith_chrom = mat_chrom_accur[dsi, 1:]
    diff_chroms = best_chrom - ith_chrom
    # 2. generate scaling factor based on chromosome performance
    #   if accuracy_i > threshold, scale factor = -a
    #     perceived as close to a maximum, hence perturbate out to prevent clustering
    #   if accuracy_i < threshold, scale factor = 1-a
    #     perceived as far from a maximum, use current best as guidance to perturb into
    #   else accuracy_i = threshold, scale_factor = threshold
    #     perceived as can be improved, but scaling factor has to be small to prevent
    #     accidental perturbation away from a potentially closeby maximum
    #     a good heuristic is to take the threshold as the scaling factor

    #   if difference_acc > threshold, scale factor = 1-a
    #     perceived as far from a maximum, use current best as guidance to perturb into
    #   if difference_acc < threshold, scale factor = -a
    #     perceived as close to a maximum, hence perturbate out to prevent clustering
    #   else difference_acc = threshold, scale_factor = threshold
    #     perceived as can be improved, but scaling factor has to be small to prevent
    #     accidental perturbation away from a potentially closeby maximum
    #     a good heuristic is to take the threshold as the scaling factor
    ith_accur = mat_chrom_accur[dsi][0] / 100
    diff_accur = best_accur - ith_accur

    # use equation to calculate scale factor:
    # (20+n)/(10+n)-1/(x+0.1n)
    # OR
    # (20+x)/(10+x)-1/(x+0.1x), x = diff_accur
    # scale_factor = 2 - 1/diff_accur
    # scale_factor = 20.5/10.5 - 1/(diff_accur + 0.05)
    # scale_factor = 21/11 - 1/(diff_accur + 0.1)
    # scale_factor = 21.5/11.5 - 1/(diff_accur + 0.15)
    scale_factor = 11/6 - 1/(diff_accur + 0.2)
    # scale_factor = 12/7 - 1/(diff_accur + 0.4)
    # if diff_accur <= 0.00000000001:
    #   scale_factor = (20+0.00000000001)/(10+0.00000000001) - 1/(0.00000000001 + 0.1*0.00000000001)
    # else:
    #   scale_factor = (20+diff_accur)/(10+diff_accur) - 1/(diff_accur + 0.1*diff_accur)

    # threshold = (best_accur - worst_accur)/2 # 0.2 # 0.5 # 0.4 # 0.6

    # # use difference between best accuracy and ith accuracy
    # if diff_accur >= threshold:
    #   scale_factor = diff_accur
    # else:
    #   scale_factor = 1 / diff_accur

    # # directly use chromosome's accuracy
    # if ith_accur > threshold:
    #   scale_factor = 0 - ith_accur
    #   # scale_factor = ith_accur - 1
    # elif ith_accur < threshold:
    #   scale_factor = 1 - ith_accur
    # else:
    #   scale_factor = threshold

    # 3. directed vector generated by scaling difference of chromosomes by scaling factor
    directed_vec = diff_chroms * scale_factor
    # 4. add directed vector to ith chromosome to obtain a new chromosome
    a_new_chrom = ith_chrom + directed_vec
    np.clip(a_new_chrom, 0, 0.99999999999, out=a_new_chrom)

    # # 1. calculate dimensional mean of best chromosome
    # best_mean = np.mean(best_chrom)
    # # 2. calculate dimensional mean of ith chromosome
    # ith_chrom = mat_chrom_accur[dsi, 1:]
    # ith_mean = np.mean(ith_chrom)
    # # 3. calculate difference between mean of best chromosome and mean of ith chromosome
    # diff_mean = best_mean - ith_mean
    # # 4. generate vector containing random units [0, 1]^

    # #   if accuracy_i > threshold, scale factor = -a
    # #     perceived as close to a maximum, hence perturbate out to prevent clustering
    # #   if accuracy_i < threshold, scale factor = 1-a
    # #     perceived as far from a maximum, use current best as guidance to perturb into
    # #   else accuracy_i = threshold, scale_factor = threshold
    # #     perceived as can be improved, but scaling factor has to be small to prevent
    # #     accidental perturbation away from a potentially closeby maximum
    # #     a good heuristic is to take the threshold as the scaling factor

    # #   if difference_acc > threshold, scale factor = 1-a
    # #     perceived as far from a maximum, use current best as guidance to perturb into
    # #   if difference_acc < threshold, scale factor = -a
    # #     perceived as close to a maximum, hence perturbate out to prevent clustering
    # #   else difference_acc = threshold, scale_factor = threshold
    # #     perceived as can be improved, but scaling factor has to be small to prevent
    # #     accidental perturbation away from a potentially closeby maximum
    # #     a good heuristic is to take the threshold as the scaling factor
    # ith_accur = mat_chrom_accur[dsi][0] / 100
    # best_accur = mat_chrom_accur[0][0] / 100
    # threshold = 0.3 # 0.5 # 0.4 # 0.6
    # # use difference between best accuracy and ith accuracy
    # diff_accur = best_accur - ith_accur
    # if diff_accur > threshold:
    #   scale_factor = 1 - ith_accur
    # elif diff_accur < threshold:
    #   scale_factor = 0 - ith_accur
    # else:
    #   scale_factor = threshold
    # # # directly use chromosome's accuracy
    # # if ith_accur > threshold:
    # #   scale_factor = 0 - ith_accur
    # #   # scale_factor = ith_accur - 1
    # # elif ith_accur < threshold:
    # #   scale_factor = 1 - ith_accur
    # # else:
    # #   scale_factor = threshold

    # # 5. directed vector generated by scaling difference of means by random vector^
    # # ^may or may not be needed depending on implementation, could also change depending on implementation
    # directed_vec = [diff_mean * scale_factor for i in range(num_chrom_params)]
    # # 6. add directed vector to ith chromosome to obtain a new chromosome
    # a_new_chrom = ith_chrom + directed_vec
    # np.clip(a_new_chrom, 0, 0.99999999999, out=a_new_chrom)

    # evaluate new chromosome
    new_res, new_chroms_list = eval_chromosomes([a_new_chrom], num_chrom_params)

    if new_res[0] > best_res[0]:
      best_res = new_res

    new_chromosomes.append(new_chroms_list[0].tolist())
    # # add new chromosome to next iteration if it is better than its previous version
    # # otherwise carry over the previous chromosome
    # if new_res[0] > mat_chrom_accur[dsi][0]:
    #   new_chromosomes.append(new_chroms_list[0].tolist())
    # else:
    #   new_chromosomes.append(mat_chrom_accur[dsi].tolist())
    
    diff_search_iter += 1

  return new_chromosomes

  # # Scan through new solutions
  # for si in range(num_new_sol):
  #   # find average between first few best chromosomes
  #   # find difference between best chromosome and the worst ones
  #   # divide the difference by the differential learning rate
  #   #  larger difference -> assumed as the generation has not plateau-ed yet (has not reached local optima)
  #   #  smaller difference -> assumed as the generation has reached local optima and needs a larger push to get out from it
  #   difference_of_chroms = best_chrom - mat_chrom_accur[len(mat_chrom_accur)-1-si, 1:]
  #   difference_of_chroms = [0.00000000001 if i == 0 else i for i in difference_of_chroms]
  #   difference_of_chroms = [(meta['diff_lr']/difference_of_chroms[i]) for i in range(num_chrom_params)]
  #   # print(difference_of_chroms)
  #   # if np.isinf(difference_of_chroms):
  #   #   while True: pass
    
  #   # Add differential whilst applying a learning rate
  #   a_new_chrom = best_chrom + difference_of_chroms
  #   # interpolate up to 3 standard deviations from mean to [0,1) and clip the outliers
  #   avg_param_value = np.mean(a_new_chrom)
  #   std_param_value = np.std(a_new_chrom)
  #   a_new_chrom = np.interp(a_new_chrom, [avg_param_value - (3 * std_param_value), avg_param_value + (3 * std_param_value)], [0, 1])
  #   np.clip(a_new_chrom, 0, 0.99999999999, out=a_new_chrom)
  #   # Store new solution 
  #   new_chromosomes.append(a_new_chrom)

  # return new_chromosomes

def differential_search(chroms_list, good_solutions, bad_solutions, num_chrom_params):
  global best_res

  new_chromosomes = []
  dsi = 0
  for i, a_chrom in enumerate(chroms_list):

    print("=========================================================")
    print("differential search iteration: {}".format(dsi))
    print("=========================================================")

    good_sol = random.choice(good_solutions)
    bad_sol = random.choice(bad_solutions)

    difference_accur = (good_sol[0] - bad_sol[0]) / 100
    difference_vec = np.subtract(good_sol[1:], bad_sol[1:])

    directed_vec = np.multiply(difference_vec, difference_accur)
    new_chrom = np.add(a_chrom[1:], directed_vec)
    
    np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)
    new_chromosomes.append(new_chrom)
    dsi += 1

  new_res, new_chrom_accur = eval_chromosomes(new_chromosomes, num_chrom_params)
  if new_res[0] > best_res[0]:# and random.random() < 0.8:  # 30% to not replace current best with new best to avoid overfitting
    best_res = new_res

  # considering changing to have elimination / production
  return new_chrom_accur


# -----------------------------------------------------------------------------
# local search functions

def local_search(a_chrom, prev_chrom, num_chrom_params):
  global best_res, local_search_iter, testing

  ####################################################################
  # ALGORITHM TO IMPLEMENT
  ####################################################################
  # local search with random walking:
  # params: current chromosome, previous chromosome, number of chromosome params
  #   obtain trajectory from difference between current chromosome and previous chromosome
  #   scale trajectory down, this will be the starting direction in which to walk
  #   accuracy will determine probability of going in roughly the trajectory direction
  #   1-accuracy will determine probability of going other places
  #   evaluate chromosome when step taken
  #   update accuracy to new one

  # accur_decline = False                               #
  initial_chrom = a_chrom
  best_chrom = a_chrom
  # best_accur = a_chrom[0]
  accuracy = a_chrom[0] / 100
  # accuracy_diff = (a_chrom[0] - prev_chrom[0]) / 100  #
  # if accuracy_diff < 0.0:                             #
  #   accuracy_diff = abs(accuracy_diff)                #
  #   accur_decline = True                              #

  max_iterations = 10 # max number of iterations to prevent algorithm being stuck in local search
  breakout_countdown = 10 - math.ceil(best_chrom[0]/10)  ##### can change  # 10 - math.ceil(initial_chrom[0]/10) # larger values get less iterations  # 10 - math.ceil(best_chrom[0]/10) # larger values get less iterations
  lsi = 0
  while breakout_countdown >= 0 and max_iterations >= 0: # for lsi in range(5):  ##### can change  # while breakout_countdown >= 0 and max_iterations >= 0:

    print("=========================================================")
    print("local search iteration: {}".format(lsi))
    print("=========================================================")

    trajectory = np.subtract(a_chrom[1:], prev_chrom[1:])
    step = trajectory * 0.1
    step_magnitude = sum(p**2 for p in step) ** .5  # (2)
    magnitude_ratio = step_magnitude / sum(1 for p in range(num_chrom_params)) ** .5  # (2)
    threshold_angle = math.pi/4 * accuracy + math.pi/4  # 45deg * accuracy + 45deg  # (2) # math.pi/2 * accuracy  # 90deg * accuracy
    limit = 10
    rand_vec = np.array([])
    
    # # the higher the accuracy change, the more likely it will follow in the trajectory
    # if random.random() < accuracy:
    #   # maintain trajectory
    #   new_chrom = np.add(a_chrom[1:], step) # (1)
    # else:
    #   # change trajectory
    #   step = np.multiply(step, -1)          # (1)
    #   new_chrom = np.add(a_chrom[1:], step) # (1)


    # the higher the accuracy change, the more likely it will follow in the trajectory  # (2)
    if random.random() < accuracy:
      # get threshold angle in radian by math.pi / 2 * accuracy (90 deg / accuracy)
      # generate random vector
      # check if angle is within threshold angle, if not repeat until it is obtained
      # limit the magnitude of the vector by the magnitude of the trajectory * 0.1
      # add the vector to a_chrom
      while limit > 0:
        limit -= 1
        # generate a random vector to be the new trajectory
        rand_vec = gen_rand_chromosome(num_chrom_param) * 0.1
        rand_vec = np.add(rand_vec, trajectory)
        # calculate the unit vector for original trajectory and the randomized vector
        # source: https://stackoverflow.com/a/2827475
        unit_vec_trajectory = trajectory / np.linalg.norm(trajectory)
        unit_vec_rand = rand_vec / np.linalg.norm(rand_vec)
        # calculate the angle between the two vectors
        angle_between_vec = np.arccos(np.clip(np.dot(unit_vec_trajectory, unit_vec_rand), -1.0, 1.0))
        # if their angle is within the threshold angle, then the randomized vector is accepted
        if angle_between_vec <= threshold_angle:
          break
        # otherwise after some time, the default trajectory will be used
      if limit <= 0:
        testing['take_previous'] += 1
        next_step = step
      else:
        testing['take_random'] += 1
        next_step = np.multiply(rand_vec, magnitude_ratio)
      new_chrom = np.add(a_chrom[1:], next_step)
      testing['limit_numbers'][limit] += 1
    else:
      # check if angle is outside of threshold angle, if not repeat until it is obtained
      # limit the magnitude of the vector by the magnitude of the trajectory * 0.1
      # add the vector to a_chrom
      while limit > 0:
        limit -= 1
        # generate a random vector to be the new trajectory
        rand_vec = gen_rand_chromosome(num_chrom_param) * 0.1
        rand_vec = np.add(rand_vec, trajectory)
        # calculate the unit vector for original trajectory and the randomized vector
        # source: https://stackoverflow.com/a/2827475
        unit_vec_trajectory = trajectory / np.linalg.norm(trajectory)
        unit_vec_rand = rand_vec / np.linalg.norm(rand_vec)
        # calculate the angle between the two vectors
        angle_between_vec = np.arccos(np.clip(np.dot(unit_vec_trajectory, unit_vec_rand), -1.0, 1.0))
        # only if their angle is within the threshold angle, then the randomized vector is accepted
        if angle_between_vec > threshold_angle:
          break
        # otherwise after some time, the default trajectory will be used
      if limit <= 0:
        testing['take_previous'] += 1
        next_step = np.multiply(step, -1)
      else:
        testing['take_random'] += 1
        next_step = np.multiply(rand_vec, magnitude_ratio)
      new_chrom = np.add(a_chrom[1:], next_step)
      testing['limit_numbers'][limit] += 1
    
    # damping trajectory
    for k in range(len(new_chrom)):
      if new_chrom[k] == 1.0:
        new_chrom[k] = 0.99999999999
      elif new_chrom[k] == 0.0:
        new_chrom[k] = 0.00000000001
      elif new_chrom[k] > 1.0:
        new_chrom[k] = new_chrom[k] - next_step[k]
        adjustment = (1.0 - new_chrom[k]) * 0.5
        new_chrom[k] += adjustment
      elif new_chrom[k] < 0.0:
        new_chrom[k] = new_chrom[k] - next_step[k]
        adjustment = new_chrom[k] * 0.5
        new_chrom[k] -= adjustment
    
    new_res, new_chrom_accur = eval_chromosomes([new_chrom], num_chrom_params)
    if new_res[0] > best_res[0] and random.random() < 0.8:  # 30% to not replace current best with new best to avoid overfitting
      best_res = new_res
    
    # if the newly evaluated chromosome performs better than the currect best chromosome
    if new_chrom_accur[0][0] > best_chrom[0]:
      best_chrom = new_chrom_accur[0]
      breakout_countdown = 10 - math.ceil(best_chrom[0]/10)  ##### can change  # 10 - math.ceil(initial_chrom[0]/10) # larger values get less iterations  # 10 - math.ceil(best_chrom[0]/10) # larger values get less iterations
    else:
      breakout_countdown -= 1
    
    # if new_chrom_accur[0][0] <= initial_chrom[0]: ##### may change to best_accur
    #   breakout_countdown -= 1
    # else:
    #   breakout_countdown = 5  ##### can change  # 10 - math.ceil(initial_chrom[0]/10) # larger values get less iterations  # 10 - math.ceil(best_chrom[0]/10) # larger values get less iterations

    max_iterations -= 1
    accuracy = new_chrom_accur[0][0] / 100
    a_chrom = new_chrom_accur[0]

    lsi += 1
    limit = 10

  return best_chrom

  # # local search:
  # # current chromosome vector, add step (learning rate) and momentum ([-1, 1])
  # # 1. initialize step
  # # 2. randomly select momentum
  # # 3. multiply step and momentum
  # # 4. add to current chromosome vector
  # # 5. evaluate new chromosome with new chromosome vector
  # # 6. if decrease in accuracy^
  # #     - move in other direction*
  # #    else
  # #     - decrease magnitude of momentum and move in same direction
  # #     - have a chance for perturbing
  # # 7. repeat step 3 to 6 until termination criteria reached
  # #
  # # ^next step would be to change momentum with weight with repect to accuracy
  # # *depending on circumstances will have to do different things
  # step_size = meta['step_size']
  # momentum = []
  # for i in range(num_chrom_params):
  #   if random.random() < 0.5:
  #     momentum.append(-1.0)
  #   else:
  #     momentum.append(1.0)
  
  # # keeps a track record of the accuracies during local search
  # acur_record = [a_chrom[0]]
  # acur_ind = 0
  
  # chromosome = np.array(a_chrom[1:])
  # chrom_result = a_chrom
  # for lsi in range(meta["local_search_iter"]):

  #   print("=========================================================")
  #   print("local search iteration: {}".format(lsi))
  #   print("=========================================================")

  #   # convert to numpy arrays
  #   momentum = np.array(momentum)

  #   # generate movement vector as the mutation vector for local search
  #   movement_vec = momentum * step_size
  #   # add the movement to the chromosome
  #   chromosome = np.add(chromosome, movement_vec)
  #   np.clip(chromosome, 0, 0.99999999999, out=chromosome)
  #   # Test validation accuracy of new chromosome
  #   new_res, new_chrom_acur = eval_chromosomes([chromosome], num_chrom_params)
  #   acur_record.append(new_chrom_acur[0][0])
  #   acur_ind += 1

  #   if new_res[0] > best_res[0]:
  #     best_res = new_res
    
  #   if new_res[0] > chrom_result[0]:
  #     chrom_result = np.concatenate(([new_res[0]], new_res[1]))

  #   if acur_record[acur_ind] > acur_record[acur_ind - 1]:
  #     # if current accuracy is better than previous accuracy
  #     if acur_ind - 2 >= 0:
  #       if acur_record[acur_ind] - acur_record[acur_ind - 1] > acur_record[acur_ind - 1] - acur_record[acur_ind - 2]:
  #         # if current iteration's accuracy improvement is better than previous iteration's
  #         momentum *= 1.1
  #       else:
  #         # if current iteration's accuracy improvement is worse than previous iteration's
  #         momentum *= 0.8
  #     # else, retain momentum
  #   else:
  #     # if current accuracy is worse than or equal to previous accuracy
  #     if acur_ind - 2 >= 0:
  #       if acur_record[acur_ind] - acur_record[acur_ind - 1] > acur_record[acur_ind - 1] - acur_record[acur_ind - 2]:
  #         # if current iteration's accuracy regression is less than previous iteration's
  #         momentum *= 1.1
  #       else:
  #         # if current iteration's accuracy regression is more than previous iteration's
  #         momentum *= -0.8
  #     else:
  #       # else, go in opposite direction
  #       momentum *= -1.0
      
  #     local_search_iter += 1

  # return chrom_result # np.concatenate(([best_res[0]], best_res[1]))


# -----------------------------------------------------------------------------
# population-based search functions

# function that mutates the parameters of the offspring chromosome
def mutate_offspring(chrom, num_chrom_params):
  for i in chrom:
    if random.random() < meta['mutation_rate']:
      i = random.random()
  
  return chrom



# -----------------------------------------------------------------------------
# final evaluation functions

def final_test(a_model):
  a_model.eval()
  test_loss = 0
  correct = 0
  test_preds = torch.zeros(0)
  first = True
  for test_data_in, test_data_out in test_loader: 

      if args['cuda']:
          test_data_in, test_data_out = test_data_in.cuda(), test_data_out.cuda()
              
      test_data_in, test_data_out = Variable(test_data_in), Variable(test_data_out)
      output = a_model(test_data_in)
      test_loss += F.nll_loss(output, test_data_out, reduction='sum').data # sum up batch loss

      pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
      if first:
        test_preds = pred
        first = False
      else:
        test_preds = torch.cat((test_preds, pred),0)

      correct += pred.eq(test_data_out.data.view_as(pred)).long().cpu().sum()

  # Print test accuracy
  test_loss /= len(test_loader.dataset)
  accuracy = 100. * correct / len(test_loader.dataset)
  print('\nTest set: Average loss: {:.4f}, Accuracy (at final epoch): {}/{} ({:.0f}%)\n'.format(
      test_loss, correct, len(test_loader.dataset), accuracy))

  return accuracy, test_loss


# -----------------------------------------------------------------------------------------------------------
# start of algorithm

starting_time = time.time() # starting time to keep track of the total time taken to complete
all_accur_valid = []  # data of validation accuracy of each trial is stored here
all_accur_final = []  # data of final accuracy of each trial is store here
all_average_loss = [[],[]] # data if average loss if both validation accuracy and final accuracy are stored here

trials = 20 # 20
for ti in range(trials):
  meta_rs_valids = []
  best_model = None
  best_chromosome = None
  best_model_accur = 0

  do_eval_iter = 0
  local_search_iter = 0
  diff_search_iter = 0

  # list to store all initial search generated chromosomes
  mat_chroms = [[0 for x in range(comp_num_chrom_param(limits)+1)] for y in range(meta['max_rs_iter'])]


  # ---------------------------------------------------------------------------------------------------------
  # initial search

  # Start with a small search
  print('Initial random search ...')
  for rsi in range(meta['max_rs_iter']):

    if args['verbose_meta']:
      print('Search iteration {}.'.format(rsi+1))

    num_chrom_param = comp_num_chrom_param(limits)
    
    a_rand_chrom = gen_rand_chromosome(num_chrom_param)
    # a_rand_chrom = np.asarray([rsi/meta['max_rs_iter']  for i in range(num_chrom_param)])
    # a_rand_chrom = np.asarray([rsi/meta['max_rs_iter'] + (random.random()*(1/meta['max_rs_iter'])) for i in range(num_chrom_param)])

    # --- Actual training
    model, train_params = prepare_model(a_rand_chrom)
    model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])
    best_valid_accur = max(valid_accurs)
    print('Best validation accuracy: {}%.'.format(best_valid_accur))

    # Store best model
    if best_valid_accur > best_model_accur:
      best_model_accur = best_valid_accur
      best_model = model
      best_train_params = train_params
      best_chromosome = a_rand_chrom

    meta_rs_valids.append(best_valid_accur)
    # adds the generated chromosome with its accuracy into the list
    mat_chroms[rsi][0] = best_valid_accur
    mat_chroms[rsi][1:] = a_rand_chrom
  
  # # sorted list of chromosomes in the initial search by performance (accuracy)
  # mat_chroms = sorted(mat_chroms, reverse=True)
  print('*****************************************************{}'.format(ti))
  print('Best accuracy after initial random search: {}'.format(best_model_accur))
  print('*****************************************************')

  if args['verbose_meta']:
    print('Best validation errors:')
    print(meta_rs_valids)

  num_chrom_params = best_chromosome.shape[0]
  next_best_initial_chrom_ind = 1

  best_res = (best_model_accur, best_chromosome, best_model, best_train_params)

  # -----------------------------------------------------------------------------------------------------------
  # population-based search

  # generate initial direction vector for each chromosome
  direction_vec = [np.random.rand(num_chrom_param)*0.2 for i in range(meta['max_rs_iter'])]
  direction_vec = [[i[j]-0.1 for j in range(len(direction_vec[0]))] for i in direction_vec] # [i.tolist() for i in direction_vec]

  good_solutions = []
  bad_solutions = []
  good_sol_thresh = 10
  bad_sol_thresh = 10 # 5
  good_sol_radius = 6 # 11.5  # 6  # impact radius for good solutions
  bad_sol_radius = 6  # 11.5  # 6  # impact radius for bad solutions

  accuracy_thresh = 12.0

  sorted_mat_chroms = sorted(mat_chroms)
  bad_solutions = sorted_mat_chroms[0:math.floor(len(sorted_mat_chroms)/2)]
  good_solutions = sorted_mat_chroms[math.floor(len(sorted_mat_chroms)/2):]

  evaluated_chroms = []

  terminate_search = False
  pop_search_iter = 0

  has_good_sol = False
  # reset_countdown = 4
  while not terminate_search:
    print("========================================================={}".format(ti))
    print("population search iteration: {}".format(pop_search_iter))
    print("=========================================================")

    # --- loosely based ant colony optimisation with pheromone and anti-pheromone mechanisms

    # # resets the population if there is no improvement for n iterations
    # if reset_countdown <= 0:
    #   # reset whole population by randomizing each chromosome
    #   mat_chroms = np.array([gen_rand_chromosome(num_chrom_params) for i in range(len(mat_chroms))])
    #   new_res, mat_chroms = eval_chromosomes(mat_chroms, num_chrom_params)
    #   if new_res[0] > best_res[0] and random.random() < 0.7:  # 30% to not replace current best with new best to avoid overfitting
    #     best_res = new_res
    #   # # replace the worst ones with the ones from the good solution
    #   # for i in range(len(good_solutions)):
    #   #   # # max 2 chromosomes replaced
    #   #   # if i >= 2:
    #   #   #   break
    #   #   mat_chroms[mat_chroms.argmin()] = np.array(good_solutions[i])
    #   reset_countdown = 4

    do_diff_search = False
    new_chromosomes = []
    for i, a_chrom in enumerate(mat_chroms):
      closest_dis = 19.4  # approximate Euclidean distance between two farthest points in the search space
      closest_good = False
      closest_bad = False

      # for j, sol in enumerate(good_solutions):
      #   if np.all(np.equal(a_chrom, sol)):
      #     continue
      #   # calculate Euclidean distance between the two chomosomes
      #   # source: https://stackoverflow.com/a/50639386
      #   euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
      #   # if the chromosome is close enough to a good solution, regardless of distance
      #   # set the good solution as the closest 
      #   if euclid_dis <= good_sol_radius and euclid_dis < closest_dis:
      #     closest_dis = euclid_dis
      #     closest_chrom = sol[1:]
      #     closest_good = True
      
      # # calculate Euclidean distance between current chromosome and good_sol / bad_sol
      # for j, sol in enumerate(bad_solutions):
      #   if closest_good:
      #     break
      #   if np.all(np.equal(a_chrom, sol)):
      #     continue
      #   # calculate Euclidean distance between the two chomosomes
      #   euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** .5
      #   # if the chromosome is within radius of a bad solution and the solution is the closest to it,
      #   # replace with new closest solution, otherwise ignore
      #   if euclid_dis <= bad_sol_radius and euclid_dis < closest_dis:
      #     closest_dis = euclid_dis
      #     closest_chrom = sol[1:]
      #     closest_bad = True
      
      # # used in order to get the gradient intersecting x=0 at 3 and x=radius at a value < 0.5, where radius is the good/bad solution radius
      # multiply_factor = 3 - closest_dis/2.2

      # if closest_good:
      #   # apply force to change direction of chromosome towards solution
      #   force_vec = np.subtract(a_chrom[1:], closest_chrom)
      #   force_vec = np.multiply(multiply_factor, force_vec)
      #   # inverse scaling to generate larger force the closer a chromosome is to a solution
      #   direction_vec[i] = np.add(direction_vec[i], force_vec)
      #   # np.clip(direction_vec[i], 0, 0.4, out=direction_vec[i])
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])
      # elif closest_bad:
      #   # apply force to change direction of chromosome away from solution
      #   force_vec = np.subtract(a_chrom[1:], closest_chrom)
      #   force_vec *= -1
      #   force_vec = np.multiply(multiply_factor, force_vec)
      #   # inverse scaling to generate larger force the closer a chromosome is to a solution
      #   direction_vec[i] = np.add(direction_vec[i], force_vec)
      #   # np.clip(direction_vec[i], 0, 0.4, out=direction_vec[i])
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])
      # else:
      #   # retain trajectory
      #   new_chrom = np.add(a_chrom[1:], direction_vec[i])

      # ------------------------------------------------------------------
      # CHANGES MADE
      # ------------------------------------------------------------------
      # all good and bad solutions will influence the current chromosome, not just the closest
      for j, sol in enumerate(good_solutions):
        if np.all(np.equal(a_chrom, sol)):
          continue
        # calculate Euclidean distance between the two chomosomes
        # source: https://stackoverflow.com/a/50639386
        euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
        diff_vec = np.subtract(sol[1:], a_chrom[1:])
        force_vec = np.divide(diff_vec, euclid_dis/5)
        direction_vec[i] = np.add(direction_vec[i], force_vec)
      
      for j, sol in enumerate(bad_solutions):
        if np.all(np.equal(a_chrom, sol)):
          continue
        # calculate Euclidean distance between the two chomosomes
        # source: https://stackoverflow.com/a/50639386
        euclid_dis = sum((p-q)**2 for p, q in zip(a_chrom[1:], sol[1:])) ** 0.5
        diff_vec = np.subtract(sol[1:], a_chrom[1:])
        diff_vec = np.multiply(diff_vec, -1)
        force_vec = np.divide(diff_vec, euclid_dis/5)
        direction_vec[i] = np.add(direction_vec[i], force_vec)

      new_chrom = np.add(a_chrom[1:], direction_vec[i])
      
      # # wrap around
      # for k in range(len(new_chrom)):
      #   if new_chrom[k] == 1.0:
      #     new_chrom[k] = 0.00000000001
      #   elif new_chrom[k] == 0.0:
      #     new_chrom[k] = 0.99999999999
      #   elif new_chrom[k] > 1.0 or new_chrom[k] < 0.0:
      #     new_chrom[k] -= math.floor(new_chrom[k])
      # # bounce back
      # for k in range(len(new_chrom)):
      #   if new_chrom[k] == 1.0:
      #     new_chrom[k] = 0.99999999999
      #   elif new_chrom[k] == 0.0:
      #     new_chrom[k] = 0.00000000001
      #   elif new_chrom[k] > 1.0 or new_chrom[k] < 0.0:
      #     new_chrom[k] -= math.ceil(new_chrom[k])

      # # ------------------------------------------------------------------
      # # CHANGES MADE
      # # ------------------------------------------------------------------
      # # damping effect - this is to: 
      # # (1) prevent overflow/underflow of param values,
      # # (2) allows chromosome to stay in roughly the same area and remain on trajectory (prevent extreme perturbation like in wraparound)
      # # (3) similar to bounce back, allows chromosome to remain around the intended area, but is more faithful to the trajectory
      # # (4) prevent getting stuck on search space as np.clip tends to do
      # for k in range(len(new_chrom)):
      #   if new_chrom[k] == 1.0:
      #     new_chrom[k] = 0.99999999999
      #   elif new_chrom[k] == 0.0:
      #     new_chrom[k] = 0.00000000001
      #   elif new_chrom[k] > 1.0:
      #     new_chrom[k] = new_chrom[k] - direction_vec[i][k]
      #     adjustment = (1.0 - new_chrom[k]) * 0.5
      #     new_chrom[k] += adjustment
      #   elif new_chrom[k] < 0.0:
      #     new_chrom[k] = new_chrom[k] - direction_vec[i][k]
      #     adjustment = new_chrom[k] * 0.5
      #     new_chrom[k] -= adjustment


      np.clip(new_chrom, 0, 0.99999999999, out=new_chrom)
      new_chromosomes.append(new_chrom)
    
    # test validation accuracy of new chromosomes
    new_res, evaluated_chroms = eval_chromosomes(new_chromosomes, num_chrom_params)
    if new_res[0] > best_res[0] and random.random() < 0.8:  # 30% to not replace current best with new best to avoid overfitting
      best_res = new_res
    
    ####################################################################
    # CHANGES TO BE MADE
    ####################################################################
    # set bad / good based on accuracy
    # - accuracy goes up as search iteration increases
    # - abolish based on previous accuracy
    # good solution radius increase for each iteration no good solutions are added
    # bad solution radius decrease for each iteration the above happened
    # revert back to original radii when a good solution is saved
    # 
    for i, a_chrom in enumerate(evaluated_chroms):
      # if solution generated is worse or equal to the threshold
      if a_chrom[0] <= accuracy_thresh:
        if len(bad_solutions) >= bad_sol_thresh:
          bad_solutions[bad_solutions.index(max(bad_solutions))] = a_chrom.tolist()
        else:
          bad_solutions.append(a_chrom.tolist())
      else:
        has_good_sol = True
        new_chrom = local_search(a_chrom, mat_chroms[i], num_chrom_params)
        evaluated_chroms[i] = new_chrom
        if len(good_solutions) >= good_sol_thresh:
          good_solutions[good_solutions.index(min(good_solutions))] = new_chrom.tolist()
        else:
          good_solutions.append(new_chrom.tolist())
    
    if not has_good_sol:
      # increase good solution radius, decrease bad solution radius
      good_sol_radius /= 0.9
      bad_sol_radius *= 0.9

      # differential search
      new_population = differential_search(evaluated_chroms, good_solutions, bad_solutions, num_chrom_params)
      do_diff_search = True

      # counter for resetting
      # reset_countdown -= 1
    else:
      # reset radius
      good_sol_radius = 6
      bad_sol_radius = 6
    
    has_good_sol = False
    accuracy_thresh += 1.0
    
    # for i, a_chrom in enumerate(evaluated_chroms):
    #   # if solution generated is worse or equal to previous solution
    #   if a_chrom[0] <= mat_chroms[i][0]:
    #     if len(bad_solutions) >= bad_sol_thresh:
    #       bad_solutions[bad_solutions.index(max(bad_solutions))] = a_chrom.tolist()
    #     else:
    #       bad_solutions.append(a_chrom.tolist())
    #   elif a_chrom[0] > mat_chroms[i][0]:
    #     # local_search(a_chrom)
    #     if len(good_solutions) >= good_sol_thresh:
    #       good_solutions[good_solutions.index(min(good_solutions))] = a_chrom.tolist()
    #     else:
    #       good_solutions.append(a_chrom.tolist())
      # if a_chrom[0] <= 12.0:
      #   if len(bad_solutions) >= bad_sol_thresh:
      #     # bad_solutions[bad_solutions.index(max(bad_solutions))] = a_chrom.tolist()
      #     bad_solutions.pop(0)
      #     bad_solutions.append(a_chrom)
      #   else:
      #     bad_solutions.append(a_chrom.tolist())
      # else:
      #   # local_search(a_chrom)
      #   if len(good_solutions) >= good_sol_thresh:
      #     # good_solutions[good_solutions.index(min(good_solutions))] = a_chrom.tolist()
      #     good_solutions.pop(0)
      #     good_solutions.append(a_chrom)
      #   else:
      #     good_solutions.append(a_chrom.tolist())
    
    if do_diff_search:
      mat_chroms = new_population
    else:
      mat_chroms = evaluated_chroms
    



    # initialize population
    # evaluate population
    # store global best
    # initialize direction_vector of each chromosome
    # while not terminating, do
    #   
    #   for each chromosome in population, do
    #     if chromosome within radius of a good solution, then
    #       apply force to change direction of chromosome towards solution
    #     else if chromosome within radius of a bad solution, then
    #       apply force to change direction of chromosome away from solution
    #       (perform differential search between current chromosome and ...)                *****
    #       (remove chromosome and add chromosome based on differential search between ...) *****
    #     else
    #       retain trajectory
    #     end if 
    #   end for
    #   
    #   evaluate newly generated chromosomes
    #   for each chromosome, do
    #     if solution is worse (or the same), then
    #       if list of bad solutions > threshold, then
    #         replace oldest (best) bad solutions with latest bad solutions
    #       else
    #         add to list of bad solutions
    #       end if
    #     else
    #       if list of good solutions > threshold, then
    #         replace oldest (worst) good solutions with latest good solutions
    #       else
    #         add to list of good solutions
    #       end if
    #       perform local search on chromosome (use random walking technique)
    #     end if
    #   end for
    #   
    #   (increase redius of good and bad solutions)
    # end while


    print('======================================{}'.format(ti))
    print('Best accuracy so far: {}'.format(best_res[0]))
    print('======================================')

    pop_search_iter += 1
      

  # -----------------------------------------------------------------------------------------------------------
  # final evaluation

  # ==========================
  # Test final model / Visualize predictions
  # ==========================

  # Final visualization
  # print('======================================')
  # print('Best accuracy so far: {}'.format(best_res[0]))
  # print('======================================')
  print('Computing the final test ...')
  
  best_model_accur, best_chromosome, best_model, best_training_params = best_res
  best_final_model, best_validation_model, best_valid_accurs, best_train_accurs = do_training(best_model, best_training_params, args['num_epochs_test'])

  print('=====================================')
  print('Training accuracies')
  print('=====================================')
  print(best_train_accurs)
  print('=====================================')
  print('Best training parameters')
  print('=====================================')
  print(best_training_params)
  print('=====================================')
  print('Best model')
  print('=====================================')
  print(best_validation_model)
  print('=====================================')
  print('Test accuracies')
  print('=====================================')
  print('Model with best validation accuracy: ')
  accur_valid, test_loss = final_test(best_validation_model)
  all_average_loss[0].append(test_loss.item())
  print('Model at the end of training: ')
  accur_final, test_loss = final_test(best_final_model)
  all_average_loss[1].append(test_loss.item())

  # max_accur = int(max(accur_valid, accur_final))
  # print("The max accuracy is: {}".format(max_accur))

  all_accur_valid.append(accur_valid)
  all_accur_final.append(accur_final)
    
valid_accuracies = [i.item() for i in all_accur_valid]
final_accuracies = [i.item() for i in all_accur_final]
print("All valid accuracies: {}".format(valid_accuracies))
print("All final accuracies: {}".format(final_accuracies))
print("All average validation loss: {}".format(all_average_loss[0]))

print("Mean accuracy:         {}".format(np.mean(valid_accuracies)))
print("Standard deviation:    {}".format(np.std(valid_accuracies)))
print("Minimum accuracy:      {}".format(min(valid_accuracies)))
print("1st Quartile accuracy: {}".format(np.percentile(valid_accuracies, 25)))
print("Median accuracy:       {}".format(np.percentile(valid_accuracies, 50)))
print("3rd Quartile accuracy: {}".format(np.percentile(valid_accuracies, 75)))
print("Maximum accuracy:      {}".format(max(valid_accuracies)))
print("Interquartile range:   {}".format(np.percentile(valid_accuracies, 75) - np.percentile(valid_accuracies, 25)))
print("Total time taken: {}".format((time.time()-starting_time)))

print("Do eval calls per trial: {}".format(do_eval_iter))
print("Take previous: {}".format(testing['take_previous']))
print("Take random: {}".format(testing['take_random']))
print("Limits: {}".format(testing['limit_numbers']))
# print("Population search iterations per trial: {}".format(pop_search_iter))
# print("Local search iterations per trial: {}".format(local_search_iter))
# print("Differential search iterations per trial: {}".format(diff_search_iter))

# --- Saving the best chromosome as a csv file
if args['save_best_chrom']:

  print('Saving the best chromosome in the root directory ...')

  from datetime import datetime
  from numpy import asarray
  from numpy import savetxt
  import numpy as np

  def get_date_time_str():
    now = datetime.now() # current date and time
    return now.strftime("%d%m%Y_%H%M")

  # Create filename
  path = './'
  filename = get_date_time_str() + "_" + str(max_accur)
  savetxt(path+filename, best_chromosome, delimiter=',')